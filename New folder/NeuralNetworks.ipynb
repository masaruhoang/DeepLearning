{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "1. ### torch.nn -> Be constructed a Neural Network\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "\n",
    "2. ### A typical training procedure for a NN is:\n",
    "    - Define the NN that has some learnable params (or Weights)\n",
    "    - iterate over a dataset of Inputs\n",
    "    - Process input through the network\n",
    "    - Computer the loss (how far is the output from being correct)\n",
    "    - Propagate gradients back into the networkâ€™s parameters\n",
    "    - Update the weights of the network, typically using a simple update rule:\n",
    "        - weight = weight - learning_rate * gradient\n",
    "        \n",
    "        \n",
    "3. ### Other\n",
    "    - x.view -> to resize or reshape a tensor\n",
    "        - Ex:  = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8)  \n",
    "            ##### -> torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
    "            \n",
    "        - #### torch.Tensor \n",
    "            - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "        - #### nn.Module \n",
    "            - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "        - #### nn.Parameter \n",
    "            - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "        - #### autograd.Function \n",
    "            - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history.         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create Net Class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square Conv Kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        \n",
    "        # 6-input, 16-output, 5x5 Conv\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) \n",
    "        \n",
    "        #An affine operation: y = Wx + b nn.Linear(in, out)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        # Reshape x to feed for Fully Connection layer\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        print(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The learnable params of a model are returned \n",
    "    - net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "\n",
    "# Conv 1's weight\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.4484, -1.5084, -1.4917,  ...,  0.2543,  1.2232,  1.0744],\n",
      "          [ 0.2502, -0.5536, -1.0087,  ...,  0.6849, -0.3433, -2.2479],\n",
      "          [-0.0577, -0.9031, -1.5983,  ..., -0.7729, -0.4029,  0.5592],\n",
      "          ...,\n",
      "          [-1.4986, -0.4177, -0.5413,  ..., -0.4576,  0.4274,  1.3754],\n",
      "          [-0.1447,  0.0334, -0.4283,  ..., -0.1088,  0.5749,  0.0719],\n",
      "          [-1.0549,  1.4417,  0.3850,  ...,  1.0538, -0.1239,  0.6827]]]])\n",
      "tensor([[ 0.4976,  0.4914,  0.4222,  0.6027,  0.3324,  0.3767,  0.5629,\n",
      "          0.5684,  0.4425,  0.5847,  0.5089,  0.2855,  0.9547,  0.2774,\n",
      "          0.3868,  0.5210,  0.3204,  0.5549,  0.4800,  0.3581,  0.6162,\n",
      "          0.7754,  0.2235,  0.3810,  0.5247,  0.2522,  0.2928,  0.1359,\n",
      "          0.0000,  0.1158,  0.1035,  0.2183,  0.2256,  0.3154,  0.1155,\n",
      "          0.1764,  0.4686,  0.0176,  0.4547,  0.4918,  0.1044,  0.3748,\n",
      "          0.0000,  0.1811,  0.3755,  0.4462,  0.3117,  0.2620,  0.4781,\n",
      "          0.3049,  0.3504,  0.2577,  0.0000,  0.0374,  0.4039,  0.4192,\n",
      "          0.1697,  0.3427,  0.0000,  0.1019,  0.4100,  0.2589,  0.0000,\n",
      "          0.5322,  0.4162,  0.2180,  0.2154,  0.1465,  0.0000,  0.0379,\n",
      "          0.0616,  0.0000,  0.3915,  0.0509,  0.0000,  0.7016,  0.6925,\n",
      "          0.9302,  0.6271,  0.8368,  1.0130,  0.9886,  0.7437,  0.8126,\n",
      "          0.5872,  0.6298,  0.6498,  0.9592,  0.7379,  0.5643,  1.1002,\n",
      "          0.7080,  0.7541,  0.7721,  0.6481,  0.8673,  0.8251,  0.7943,\n",
      "          0.7525,  0.5136,  0.4465,  0.2966,  0.0000,  0.4357,  0.0956,\n",
      "          0.0000,  0.0000,  0.0000,  0.1636,  0.0228,  0.0967,  0.0096,\n",
      "          0.0000,  0.1861,  0.0814,  0.3344,  0.0352,  0.0000,  0.0170,\n",
      "          0.0976,  0.3294,  0.0516,  0.2658,  0.0477,  0.0589,  0.9847,\n",
      "          1.0480,  1.0129,  1.1860,  1.0032,  1.2985,  1.3392,  1.1013,\n",
      "          1.0744,  0.9096,  1.1987,  1.0295,  0.8567,  0.7416,  1.0694,\n",
      "          1.0674,  1.0847,  1.0586,  0.9360,  0.6728,  0.9684,  0.9336,\n",
      "          0.9197,  1.0448,  0.8310,  0.3641,  0.5448,  0.4799,  0.4815,\n",
      "          0.7340,  0.2876,  0.5614,  0.6477,  0.6411,  0.5931,  0.3199,\n",
      "          0.7568,  0.7536,  0.5608,  0.4683,  0.6708,  0.4694,  0.4004,\n",
      "          0.5619,  0.6628,  0.4686,  1.0845,  0.8927,  0.6330,  0.6173,\n",
      "          1.1210,  0.8644,  1.1630,  0.9287,  1.1108,  0.9132,  1.1399,\n",
      "          1.1338,  1.0691,  1.0844,  1.1281,  1.1816,  1.0999,  1.1878,\n",
      "          1.0498,  1.5537,  1.0923,  0.8478,  1.0657,  1.0343,  1.1053,\n",
      "          1.1969,  0.8855,  0.8348,  1.2498,  0.0000,  0.1399,  0.4084,\n",
      "          0.0000,  0.0598,  0.0000,  0.0374,  0.0465,  0.0000,  0.0000,\n",
      "          0.0150,  0.1708,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.2277,  0.0000,  0.0886,\n",
      "          0.0000,  0.2469,  0.2527,  0.5043,  0.1256,  0.0000,  0.0001,\n",
      "          0.2566,  0.1751,  0.4584,  0.0360,  0.3694,  0.1747,  0.2881,\n",
      "          0.2021,  0.4002,  0.3578,  0.1183,  0.0313,  0.2293,  0.1257,\n",
      "          0.0149,  0.2373,  0.3348,  0.6018,  0.2434,  0.1715,  0.4706,\n",
      "          0.4622,  0.2919,  0.4166,  0.2797,  0.4414,  0.4715,  0.2086,\n",
      "          0.4719,  0.2929,  0.4264,  0.3354,  0.2102,  0.3180,  0.0000,\n",
      "          0.1602,  0.3899,  0.3760,  0.1545,  0.1945,  0.1523,  0.1493,\n",
      "          0.2954,  0.1012,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0352,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0723,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6422,\n",
      "          0.6001,  0.8777,  0.6564,  0.9161,  0.8350,  0.9171,  0.9743,\n",
      "          1.0391,  0.8364,  0.7780,  0.8348,  0.9262,  0.7962,  0.7533,\n",
      "          0.8825,  0.7436,  0.6918,  0.9504,  0.9973,  0.7561,  1.2126,\n",
      "          0.7806,  0.9126,  0.8503,  0.0000,  0.0000,  0.0996,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0111,  0.0000,  0.0000,  0.0000,\n",
      "          0.0121,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0950,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0148,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2852,  0.0000,\n",
      "          0.0000,  0.0000,  0.0439,  0.0000,  0.2158,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0073,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1124]])\n",
      "tensor([[-0.0364,  0.0273, -0.0773,  0.0369,  0.0196, -0.1248, -0.0160,\n",
      "          0.1464,  0.0161, -0.0664]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "print(input )\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero the gradient buffers of all parameters and backprops with random gradients:\n",
    "    - torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Funtion\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4976,  0.4914,  0.4222,  0.6027,  0.3324,  0.3767,  0.5629,\n",
      "          0.5684,  0.4425,  0.5847,  0.5089,  0.2855,  0.9547,  0.2774,\n",
      "          0.3868,  0.5210,  0.3204,  0.5549,  0.4800,  0.3581,  0.6162,\n",
      "          0.7754,  0.2235,  0.3810,  0.5247,  0.2522,  0.2928,  0.1359,\n",
      "          0.0000,  0.1158,  0.1035,  0.2183,  0.2256,  0.3154,  0.1155,\n",
      "          0.1764,  0.4686,  0.0176,  0.4547,  0.4918,  0.1044,  0.3748,\n",
      "          0.0000,  0.1811,  0.3755,  0.4462,  0.3117,  0.2620,  0.4781,\n",
      "          0.3049,  0.3504,  0.2577,  0.0000,  0.0374,  0.4039,  0.4192,\n",
      "          0.1697,  0.3427,  0.0000,  0.1019,  0.4100,  0.2589,  0.0000,\n",
      "          0.5322,  0.4162,  0.2180,  0.2154,  0.1465,  0.0000,  0.0379,\n",
      "          0.0616,  0.0000,  0.3915,  0.0509,  0.0000,  0.7016,  0.6925,\n",
      "          0.9302,  0.6271,  0.8368,  1.0130,  0.9886,  0.7437,  0.8126,\n",
      "          0.5872,  0.6298,  0.6498,  0.9592,  0.7379,  0.5643,  1.1002,\n",
      "          0.7080,  0.7541,  0.7721,  0.6481,  0.8673,  0.8251,  0.7943,\n",
      "          0.7525,  0.5136,  0.4465,  0.2966,  0.0000,  0.4357,  0.0956,\n",
      "          0.0000,  0.0000,  0.0000,  0.1636,  0.0228,  0.0967,  0.0096,\n",
      "          0.0000,  0.1861,  0.0814,  0.3344,  0.0352,  0.0000,  0.0170,\n",
      "          0.0976,  0.3294,  0.0516,  0.2658,  0.0477,  0.0589,  0.9847,\n",
      "          1.0480,  1.0129,  1.1860,  1.0032,  1.2985,  1.3392,  1.1013,\n",
      "          1.0744,  0.9096,  1.1987,  1.0295,  0.8567,  0.7416,  1.0694,\n",
      "          1.0674,  1.0847,  1.0586,  0.9360,  0.6728,  0.9684,  0.9336,\n",
      "          0.9197,  1.0448,  0.8310,  0.3641,  0.5448,  0.4799,  0.4815,\n",
      "          0.7340,  0.2876,  0.5614,  0.6477,  0.6411,  0.5931,  0.3199,\n",
      "          0.7568,  0.7536,  0.5608,  0.4683,  0.6708,  0.4694,  0.4004,\n",
      "          0.5619,  0.6628,  0.4686,  1.0845,  0.8927,  0.6330,  0.6173,\n",
      "          1.1210,  0.8644,  1.1630,  0.9287,  1.1108,  0.9132,  1.1399,\n",
      "          1.1338,  1.0691,  1.0844,  1.1281,  1.1816,  1.0999,  1.1878,\n",
      "          1.0498,  1.5537,  1.0923,  0.8478,  1.0657,  1.0343,  1.1053,\n",
      "          1.1969,  0.8855,  0.8348,  1.2498,  0.0000,  0.1399,  0.4084,\n",
      "          0.0000,  0.0598,  0.0000,  0.0374,  0.0465,  0.0000,  0.0000,\n",
      "          0.0150,  0.1708,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.2277,  0.0000,  0.0886,\n",
      "          0.0000,  0.2469,  0.2527,  0.5043,  0.1256,  0.0000,  0.0001,\n",
      "          0.2566,  0.1751,  0.4584,  0.0360,  0.3694,  0.1747,  0.2881,\n",
      "          0.2021,  0.4002,  0.3578,  0.1183,  0.0313,  0.2293,  0.1257,\n",
      "          0.0149,  0.2373,  0.3348,  0.6018,  0.2434,  0.1715,  0.4706,\n",
      "          0.4622,  0.2919,  0.4166,  0.2797,  0.4414,  0.4715,  0.2086,\n",
      "          0.4719,  0.2929,  0.4264,  0.3354,  0.2102,  0.3180,  0.0000,\n",
      "          0.1602,  0.3899,  0.3760,  0.1545,  0.1945,  0.1523,  0.1493,\n",
      "          0.2954,  0.1012,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0352,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0723,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6422,\n",
      "          0.6001,  0.8777,  0.6564,  0.9161,  0.8350,  0.9171,  0.9743,\n",
      "          1.0391,  0.8364,  0.7780,  0.8348,  0.9262,  0.7962,  0.7533,\n",
      "          0.8825,  0.7436,  0.6918,  0.9504,  0.9973,  0.7561,  1.2126,\n",
      "          0.7806,  0.9126,  0.8503,  0.0000,  0.0000,  0.0996,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0111,  0.0000,  0.0000,  0.0000,\n",
      "          0.0121,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0950,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0148,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2852,  0.0000,\n",
      "          0.0000,  0.0000,  0.0439,  0.0000,  0.2158,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0073,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1124]])\n",
      "tensor([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])\n",
      "tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]])\n",
      "tensor(38.5406)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "\n",
    "# a dummy targer\n",
    "target = torch.arange(1, 11)\n",
    "print(target)\n",
    "\n",
    "target = target.view(1, -1)\n",
    "print(target)\n",
    "\n",
    "#Make it the same shape ouput\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0018,  0.1106, -0.0105, -0.0509, -0.0380, -0.0118])\n"
     ]
    }
   ],
   "source": [
    "# zeroes the gradient buffers of all params\n",
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights\n",
    " \n",
    "### Stochastic Gradient Descent (SGD):\n",
    "        weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SGD\n",
    "However, as use NN, want to use various different update rules as SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4847,  0.4703,  0.4000,  0.5825,  0.3136,  0.3630,  0.5433,\n",
      "          0.5407,  0.4286,  0.5630,  0.4807,  0.2699,  0.9264,  0.2530,\n",
      "          0.3739,  0.4964,  0.2937,  0.5327,  0.4662,  0.3328,  0.5934,\n",
      "          0.7545,  0.1964,  0.3512,  0.5065,  0.3052,  0.3521,  0.2007,\n",
      "          0.0351,  0.1709,  0.1621,  0.2872,  0.2878,  0.3645,  0.1765,\n",
      "          0.2361,  0.5216,  0.0845,  0.5150,  0.5490,  0.1754,  0.4478,\n",
      "          0.0000,  0.2298,  0.4402,  0.5143,  0.3890,  0.3306,  0.5364,\n",
      "          0.3649,  0.3201,  0.2181,  0.0000,  0.0065,  0.3663,  0.3863,\n",
      "          0.1325,  0.3097,  0.0000,  0.0746,  0.3769,  0.2182,  0.0000,\n",
      "          0.4910,  0.3860,  0.1884,  0.1710,  0.1164,  0.0000,  0.0000,\n",
      "          0.0245,  0.0000,  0.3442,  0.0093,  0.0000,  0.7442,  0.7436,\n",
      "          0.9840,  0.6688,  0.8794,  1.0516,  1.0383,  0.7922,  0.8648,\n",
      "          0.6246,  0.6750,  0.7099,  1.0075,  0.7855,  0.6067,  1.1525,\n",
      "          0.7542,  0.7983,  0.8161,  0.6916,  0.9079,  0.8716,  0.8460,\n",
      "          0.8065,  0.5528,  0.4540,  0.2959,  0.0000,  0.4328,  0.0880,\n",
      "          0.0000,  0.0000,  0.0000,  0.1714,  0.0232,  0.0866,  0.0133,\n",
      "          0.0000,  0.1840,  0.0829,  0.3380,  0.0293,  0.0000,  0.0177,\n",
      "          0.0906,  0.3316,  0.0486,  0.2633,  0.0551,  0.0514,  1.0391,\n",
      "          1.1099,  1.0848,  1.2443,  1.0538,  1.3745,  1.4128,  1.1666,\n",
      "          1.1363,  0.9710,  1.2717,  1.0842,  0.9152,  0.8024,  1.1347,\n",
      "          1.1362,  1.1490,  1.1120,  0.9980,  0.7479,  1.0336,  1.0053,\n",
      "          0.9884,  1.1060,  0.8894,  0.4207,  0.6059,  0.5399,  0.5454,\n",
      "          0.7828,  0.3421,  0.6236,  0.7133,  0.7046,  0.6468,  0.3858,\n",
      "          0.8155,  0.8182,  0.6256,  0.5204,  0.7324,  0.5307,  0.4745,\n",
      "          0.6235,  0.7261,  0.5253,  1.1466,  0.9597,  0.6919,  0.6819,\n",
      "          1.0529,  0.7888,  1.0933,  0.8617,  1.0396,  0.8361,  1.0593,\n",
      "          1.0578,  1.0058,  1.0169,  1.0571,  1.1029,  1.0275,  1.1184,\n",
      "          0.9736,  1.4623,  1.0307,  0.7744,  0.9970,  0.9595,  1.0332,\n",
      "          1.1187,  0.7972,  0.7709,  1.1776,  0.0000,  0.1365,  0.4202,\n",
      "          0.0000,  0.0680,  0.0000,  0.0455,  0.0632,  0.0000,  0.0000,\n",
      "          0.0108,  0.1836,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.2377,  0.0000,  0.0944,\n",
      "          0.0000,  0.3178,  0.3393,  0.5939,  0.1948,  0.0504,  0.0667,\n",
      "          0.3402,  0.2657,  0.5390,  0.1126,  0.4350,  0.2604,  0.3697,\n",
      "          0.2840,  0.4738,  0.4346,  0.2009,  0.1202,  0.3042,  0.2064,\n",
      "          0.0977,  0.3267,  0.4244,  0.6756,  0.3196,  0.1024,  0.3968,\n",
      "          0.3911,  0.2136,  0.3456,  0.2078,  0.3707,  0.3906,  0.1390,\n",
      "          0.4033,  0.2172,  0.3526,  0.2633,  0.1405,  0.2510,  0.0000,\n",
      "          0.0956,  0.3151,  0.3012,  0.0892,  0.1340,  0.0640,  0.0661,\n",
      "          0.2246,  0.0389,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0479,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0768,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6254,\n",
      "          0.5765,  0.8492,  0.6416,  0.9000,  0.8118,  0.8985,  0.9557,\n",
      "          1.0142,  0.8128,  0.7609,  0.8132,  0.9074,  0.7761,  0.7333,\n",
      "          0.8452,  0.7159,  0.6734,  0.9351,  0.9729,  0.7407,  1.1808,\n",
      "          0.7535,  0.8877,  0.8262,  0.0000,  0.0000,  0.1210,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0358,  0.0000,  0.0000,  0.0000,\n",
      "          0.0306,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1060,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0524,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3268,  0.0000,\n",
      "          0.0394,  0.0000,  0.0891,  0.0000,  0.2499,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0389,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.1421]])\n"
     ]
    }
   ],
   "source": [
    "# Create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr= 0.01)\n",
    "\n",
    "# In training loop:\n",
    "optimizer.zero_grad() # Zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
