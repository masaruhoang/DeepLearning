{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テンソルを作成\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]),requires_grad=True)\n",
    "b = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "\n",
    "# 計算グラフを構築\n",
    "# y = 2 * x + 3\n",
    "y = w*x + b\n",
    "\n",
    "# 勾配を計算\n",
    "y.backward()\n",
    "\n",
    "# 勾配を表示\n",
    "print(x.grad)  # dy/dx = w = 2\n",
    "print(w.grad)  # dy/dw = x = 1\n",
    "print(b.grad)  # dy/db = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例１\n",
    "\n",
    "\\begin{align} \n",
    "y = x^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{dy}{dx} = 2x\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例 2\n",
    "\n",
    "\\begin{align} \n",
    "y = e^x\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{dy}{dx} = e^x\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.exp(x)  # e ~ 2.71828\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例 3\n",
    "\n",
    "\\begin{align} \n",
    "y = \\sin(x)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{dy}{dx} = \\cos(x)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([np.pi]), requires_grad=True)\n",
    "y = torch.sin(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例 4\n",
    "\n",
    "\\begin{align} \n",
    "y = (x - 4)(x^2 + 6)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{dy}{dx} = 3 x^2 - 8 x + 6\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([0]), requires_grad=True)\n",
    "y = (x - 4) * (x ** 2 + 6)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例 5\n",
    "\n",
    "\\begin{align} \n",
    "y = (\\sqrt{x} + 1)^3\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{dy}{dx} = \\frac{3 (\\sqrt{x} + 1)^2}{2 \\sqrt{x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = (torch.sqrt(x) + 1) ** 3\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例 6\n",
    "\n",
    "\\begin{align} \n",
    "z = (x + 2 y)^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{\\partial z}{\\partial x} = 2 (x + 2y)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\displaystyle \\frac{\\partial z}{\\partial y} = 4 (x + 2y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "z = (x + 2 * y) ** 2\n",
    "z.backward()\n",
    "print(x.grad)  # dz/dx\n",
    "print(y.grad)  # dz/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lossを微分する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "tensor([[ 0.3974, -0.1892, -0.2784],\n",
      "        [ 0.3327, -0.3742,  0.5401]])\n",
      "b: Parameter containing:\n",
      "tensor([-0.5308, -0.5098])\n"
     ]
    }
   ],
   "source": [
    "# バッチサンプル数=5、入力特徴量の次元数=3\n",
    "x = Variable(torch.randn(5,3))\n",
    "# バッチサンプル数=5、出力特徴量の次元数=2\n",
    "y = Variable(torch.randn(5, 2))\n",
    "\n",
    "# Linear層を作成\n",
    "# 3ユニット => 2ユニット\n",
    "linear = nn.Linear(3,2)\n",
    "\n",
    "# Linear層のパラメータ\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSELoss\n",
    "\n",
    "- Creates a criterion that measures the mean squared error between n elements in the input x and target y.\n",
    "\n",
    "\n",
    "\\begin{align} \n",
    "\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "l_n = \\left( x_n - y_n \\right)^2,\n",
    "\\end{align}\n",
    "\n",
    "- where N is the batch size. If reduce is True, then:\n",
    "\\begin{split}\\ell(x, y) = \\begin{cases}\n",
    "    \\operatorname{mean}(L), & \\text{if}\\; \\text{size_average} = \\text{True},\\\\\n",
    "    \\operatorname{sum}(L),  & \\text{if}\\; \\text{size_average} = \\text{False}.\n",
    "\\end{cases}\\end{split}\n",
    "\n",
    "- The sum operation still operates over all the elements, and divides by n.\n",
    "\n",
    "# CrossEntropyLoss\n",
    "- This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "\n",
    "- The input is expected to contain scores for each class.\n",
    "\n",
    "- input has to be a Tensor of size either (minibatch,C) or (minibatch,C,d1,d2,...,dK) with K≥2 for the K-dimensional case (described later).\n",
    "- The loss can be described as:\n",
    "\\begin{split}\\ell(x, y) = \\begin{cases}\n",
    "\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "               = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\n",
    "\\end{cases}\\end{split}\n",
    "\n",
    "or in the case of the weight argument being specified:\n",
    "\n",
    "\\begin{align} \n",
    "\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.2442)\n"
     ]
    }
   ],
   "source": [
    "# lossとoptimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward\n",
    "pred = linear(x)\n",
    "\n",
    "# loss = L\n",
    "loss = criterion(pred,Variable(y.float()))\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw: tensor([[ 0.1615, -0.4457, -0.1647],\n",
      "        [ 0.1898, -1.3372, -0.4121]])\n",
      "dL/db: tensor([-0.1185, -0.8956])\n"
     ]
    }
   ],
   "source": [
    "# 勾配を表示\n",
    "print('dL/dw:', linear.weight.grad)\n",
    "print('dL/db:', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** by hand\n",
      "tensor([[ 0.5757, -0.0990, -0.0727],\n",
      "        [ 0.4901, -0.5506, -0.4168]])\n",
      "tensor([ 0.4210, -0.3707])\n"
     ]
    }
   ],
   "source": [
    "# 勾配を用いてパラメータを更新\n",
    "print('*** by hand')\n",
    "print(linear.weight.sub(0.01 * linear.weight.grad))\n",
    "print(linear.bias.sub(0.01 * linear.bias.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配降下法\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** by optimizer.step()\n",
      "Parameter containing:\n",
      "tensor([[ 0.5757, -0.0990, -0.0727],\n",
      "        [ 0.4901, -0.5506, -0.4168]])\n",
      "Parameter containing:\n",
      "tensor([ 0.4210, -0.3707])\n"
     ]
    }
   ],
   "source": [
    "# 1ステップ更新後のパラメータを表示\n",
    "# 上の式と結果が一致することがわかる\n",
    "print('*** by optimizer.step()')\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
