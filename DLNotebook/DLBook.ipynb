{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TRAINING \n",
    "<u>HuyHT5_DR 20180817</u>\n",
    " \n",
    "### 1. Tasks: \n",
    "    * Tìm hiểu về các mạng CNN\n",
    "    * Implement Chainer\n",
    "\n",
    "\n",
    "### 2. On-going Tasks \n",
    "\n",
    "    * Overview\n",
    "        * Alexnet (2012) \n",
    "        * VGG (2014) \n",
    "        * Resnet (2015) \n",
    "        * GoogleNet-Inception\n",
    "        * SqueezeNet (2016)\n",
    "    \n",
    "    * Model Processing\n",
    "        * Prunning CNN \n",
    "        \n",
    "    * Other \n",
    "        * Feature Maps\n",
    "        * Vanishing / Exploding Gradient Problems \n",
    "        * Bath Nomarlization (updated 2018-08-17)\n",
    "        * Bottleneck Layer\n",
    "        * L1/L2 Mean (not yet)\n",
    "        \n",
    "    * Basic Chainer \n",
    "        * Phần Guides\n",
    "        * Write Neural Networks By Chainer \n",
    "            * Alexnet (Chainer)\n",
    "            * VGG16 (Chainer)\n",
    "            * Resnet (Chainer)\n",
    "            * Inception-v3 (Chainer)\n",
    "            * SqueezeNet (Full_Code Pytorch)\n",
    "            * \n",
    "    * Combine study and implement model by Chainer\n",
    "\n",
    "\n",
    "### 3. Questions \n",
    "\n",
    "\t1.Tỉ lệ ratio Dropout trong mạng (F.dropout(h, ratio=0.5, train=train)) có thể thay đổi không? Có thể Thay đổi bao nhiêu để tăng hiệu quả giảm thiểu overfitting? (HuyHT5 - 2018-08-15)\n",
    "   ---> A.  Dropout thay đổi đc. Giá trị thông thường là 0.3-0.7. Giá trị càng cao (drop nhiều) thì càng ít overfit, nhưng risk mất thông tin dẫn đến underfitting **(A. Nghĩa - 2018-08-15)**\n",
    "    \n",
    "    2.Từ mạng Alex, tác giả đã dùng kernel size là 11x11 cho lớp Conv đầu tiên và 5x5 cho Conv tiếp theo. Đến các VGG và ResNet, tác giả đã chọn kernel size là 3x3 (Is thís best thing?). Và Inception-V3 là dùng cả 1x1, 3x3, 5x5 và 7x7\n",
    "    Chọn loại Convolution (Kernel size) với 11x11 ,1x1 or 3x3 or 5x5 or 7x7? Tại sao không chọn cả tất cả? Nếu không tốt, tại sao nó lại không tốt? (HuyHT5 - 2018-08-16)\n",
    "    \n",
    "   ---> Kernel size thế nào là best hiện chưa ai chứng minh được. Các con số họ đưa ra hầu hết dựa vào kết quả thực nghiệm. Việc kernel size lớn thì chi phí tính toán tăng là đúng. Tuy nhiên dùng kernel nhỏ cũng chưa hẳn đã tốt, vì kernel to biểu diễn đc hàm số phức tạp hơn. Kernel 1x1 có nhiệm vụ đặc biệt là làm giảm độ sâu ở lớp kế tiếp. Kernel nhỏ nhất thường dùng là 3x3. **(A. Nghĩa - 2018-08-16)**\n",
    "   \n",
    "    3. Weight Initialization là vấn đề trong Network Train và là vấn đề lớn khi mình có một Deeper Network.\n",
    "    \n",
    "    - Tại sao Batch Normarlization giúp dễ  dàng trong việc khởi tạo Weights cho Network?\n",
    "    \n",
    "    4. Hiện tại có nhiều cách để giúp khởi tạo Weights như 1: zero initialization, random initialization, he-et-al initialization.\n",
    "    \n",
    "    - Việc Weight Initialization ảnh hưởng như thế nào đến Network.\n",
    "    \n",
    "    Ex: Nêú khởi tạo tất cả trọng số bằng 0 cho tất cả layers cùng giống về cách tính toán. Các weights cũng sẽ giống nhau và output sẽ cho ra kết quả luôn luôn giống nhau. (Em hiểu như vậy có đúng không ạ?)\n",
    "    \n",
    "    5. Việc đặt decay learning rate như thế nào tốt cho vấn đề hội tụ của gradient. Ví dụ, decay learning rate cho mỗi epoch với tỉ lệ giảm 0.96. hay mỗi 5, 10, or 20\n",
    "\n",
    "\n",
    "- Idead của ReLU: việc tính đạo hàm đơn giản nên giúp mạng nhanh hơn. Qua activation để nolinear. Và tính đạo hàm tại Backgro. Giảm vanishing radient problem. Tính toán của ReLU rất đơn giản.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "# --------------------------Alexnet--------------------------\n",
    "![title](Doc/alex.png)\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "    - Input: 227x227x3 (Kích thước images sẽ được crop)\n",
    "    - CONV Layers: 5conv (Phần Feature Extractor)\n",
    "    - Fully Connection layers: 3 fc (Phần Classifier)\n",
    "    - FC6 (4096 nodes) → FC7 (4096 nodes)\n",
    "    - Ouput: 1000 (Sử dụng Softmax với 1000 output classes )\n",
    "    \n",
    "#### Parameters\n",
    "    - Kiến trúc mạng Alexnet có 60 triệu params  (Using ReLU activation func)\n",
    "    \n",
    "#### Hightlights\n",
    "\n",
    "    - Thay vì sử dụng Sigmoid hoặc Tanh func thì, tác giả đã dùng ReLU activation func. Nó làm tăng tốc hơn gấp 5 lần so với các activation func khác vì khi tính đạo hàm của ReLU đơn giản hơn nhiều.\n",
    "    \n",
    "    - Với số lượng tham số lớn với 60 triệu params trong mạng, để giảm thiểu overfitting, ngoài cách thức tăng lượng data (Data Augmentation), thì tác giả đã sử dụng Dropout. Nó sẽ set về zero   ngẫu nhiên các node trong Hidden Layer with xác xuất 0.5. Dropout nó sẽ bỏ qua update lại weight vì không làm  gì cả trong khi backpropagation. Công nghệ này giảm thiểu độ phức tạp của mạng.\n",
    "    \n",
    "#### Experiment\n",
    "\n",
    "ILSVRC-2012 dataset(1000 classes): \n",
    "\n",
    "    - Training (1.3M images)\n",
    "    - Validation (50K images)\n",
    "    - Testing (100K images with held-out class labels). \n",
    "\n",
    "#### Result:\n",
    "\n",
    "    - Top 5 Error of AlexNet: 15.3%\n",
    "#### Reference\n",
    "ImageNet Classification with Deep Convolutional Neural Networks\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------VGG----------------------------\n",
    "\n",
    "![title](Doc/vgg.png)\n",
    "#### Model Architecture:\n",
    "    - Input: 224x224x3 (Kích thước images sẽ được crop)\n",
    "    \n",
    "    - Tại lớp Feature Extractor: Cải thiện hơn Alexnet với việc biểu diễn đơn giản hơn bằng cách chỉ sử dụng Lớp Conv  3x3 để tăng độ sâu cho mỗi lần output. Sau đó, giảm kích thước cho mỗi output của các lớp Conv bằng cách sử dụng Max Pooling \n",
    "    \n",
    "    - Tại Classifier: có 2 lớp Fully Connected layer (FC) với:\n",
    " \t\tFC1(4096 nodes) → FC2 (4096 nodes) → Output (1000 classes)\n",
    "\n",
    "#### Complexity\n",
    "<img src=\"Doc/vgg.jpg\" alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "                Với A ,B ,C là những điều chỉnh cho số lớp. \n",
    "#### <u>Ex</u>: \n",
    "Vgg16 với kiến trúc tại cột C Configuration, Vgg19 với cột E  Configuration\n",
    "\n",
    "#### Number of Parameters\n",
    "\n",
    "![title](Doc/vggp.jpg)\n",
    "\n",
    "                Parameters của mỗi kiến trúc (đơn vị là Milions)\n",
    "                \n",
    "#### Experiments\n",
    "ILSVRC-2012 dataset(1000 classes): \n",
    "\n",
    "    - Training (1.3M images)\n",
    "    - Validation (50K images)\n",
    "    - Testing (100K images with held-out class labels). \n",
    "\n",
    "#### Result\n",
    "![title](Doc/exp.jpg)\n",
    "\n",
    "#### Reference\n",
    "\n",
    "Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
   ]
  },
  {
   "attachments": {
    "resnetproblem.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------Resnet-------------------------\n",
    "\n",
    "<img src=\"Doc/r.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "\n",
    "![title](Doc/r.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### What problem does ResNet solve?\n",
    "    * Có môt vấn đề  là:\n",
    "        \n",
    "        \"When deeper networks starts converging with the network depth increasing, accuracy gets saturated and then degrades rapidly.\"\n",
    "        \n",
    "![resnetproblem.png](attachment:resnetproblem.png)\n",
    "        \n",
    "#### To Solve\n",
    "* Thay vì học trực tiếp từ Input -> Output với H(x) functions. Resnet định nghĩa Residual function dưới dạng:\n",
    "\n",
    "    F(x) = H(x) — x or H(x) = F(x)+x , với:\n",
    "    \n",
    "        * Hàm F(x): x là các lớp phi tuyến xếp chồng lên nhau\n",
    "\n",
    "![title](Doc/restnet1.png)\n",
    "\n",
    "***Sumup: Các model mạng càng sâu, thì các khối sẽ Conv theo chiều sâu của mạng trở thành các mạng nhỏ riêng biệt. Điều này gây ra việc các Output của các khôi này sẽ giống nhau. Ý tưởng ở đây là tác giả sử dụng lượng thông tin x + output (F(x)) của để giữ không cho thông tin gốc bị mất đi. Sau đó, đưa vào khối tiếp theo.\n",
    "           \n",
    "#### Kinds of residual connections\n",
    "\n",
    "* Residual block function when input and output dimensions are same\n",
    "![title](Doc/ct1.png)\n",
    "\n",
    "* Residual block function when the input and output dimensions are not same\n",
    "![title](Doc/ct2.png)\n",
    ".        \n",
    "                \n",
    "#### Experiments\n",
    "* ILSVRC-2015 dataset\n",
    "* COCO 2015 \n",
    "\n",
    "#### Result\n",
    "![title](Doc/resn.png)\n",
    "\n",
    "    * Top-5 error rate: 3.57% \n",
    "    * Thay thế VGG-16 layers trong Faster R-CNN sử dụng ResNet-101. Mức cải thiện lên 28%\n",
    "    * Training hiệu quả với cả 100 layers và 1000 layers\n",
    "\n",
    "#### Reference\n",
    "Understanding and Implementing Architectures of ResNet\n",
    "\n",
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------GoogleNet Inception-V3-----------------------\n",
    "\n",
    "![title](Doc/googlen1.png)\n",
    "\n",
    "## Inspiration\n",
    "\n",
    "Sau mạng Resnet152 rất sâu ra đời, thì theo xu hướng sáng tạo về mặt công nghệ. Google muốn tạo ra một mạng sâu hơn nửa.\n",
    "    \n",
    "## Problem\n",
    " 1. Chọn loại Convolution (Kernel size) 3x3 or 5x5? Tại sao không chọn cả tất cả? \n",
    " \n",
    "## Architecture\n",
    "\n",
    "![title](Doc/googlen2.png)\n",
    "\n",
    "Tác giả sử dụng cả Kernel 1x1, 3x3, 5x5 cùng với max pooling size 3x3.\n",
    "\n",
    "Tác giả cũng đề cập đến kernel 1x1 để giảm kích thước của Feature Maps của nó vì với kernel size lớn sẽ dẫn đến chi phí chi tính toán cũng lớn hơn. \n",
    "\n",
    "Đưa các kết quả feature maps (1x1 Kernel) vào reLU, sau đó làm việc với các kernel lớn hơn là (3x3 or 5x5) \n",
    "\n",
    "## How to reduce dimensionality \n",
    "\n",
    "Sử dụng kernel 1x1 để giảm kích thước của Input đến các Conv lớn, do đó nó có thể giữ hợp lí cho việc tính toán.\n",
    "\n",
    "Bảng 1.1\n",
    "![title](Doc/googlen3.png)\n",
    "    \n",
    "\n",
    "    ## 1. Chúng ta có thể xem xét về bài toán:\n",
    "![title](Doc/googlen4.png)\n",
    "\n",
    "  ### <u>Input 28x28x192 - Kernel 5x5 - Feature Maps 32x28x28:</u>\n",
    "  ### Với 5^2(28)^2(192)(32)=120,422,400 operations.\n",
    "\n",
    "\n",
    "    #2. Đới với trường hợp sau:\n",
    "\n",
    "![title](Doc/googlen5.png)\n",
    "\n",
    "Với kernel 1x1 đặt trước kernel 5x5 và output là 28x28x16 (tra tại bảng 1.1) \n",
    "![title](Doc/googlen6.png)\n",
    "\n",
    "Và chi phí tính toàn giảm khoảng 10 lần so với kiến trúc model tuyền thống.\n",
    "\n",
    "## What is the auxiliary classifiers?\n",
    "\n",
    "![title](Doc/googlen7.png)\n",
    "\n",
    "GoogleNet có 9 modules xếp chồng tuyến tính lên nhau:\n",
    "\n",
    "    * 22 layers deep (là 27 layers nếu tính cả pooling layers)\n",
    "    * Đây là bộ Deep classifier\n",
    "    \n",
    "Inception là Bộ phân loại rất sâu, nó dẫn đến tình trạng Vanishing Gradient Problem như bất kì các mạng khác.\n",
    "\n",
    "Để ngăn chặn 3 phần của GoogleNet bị **Dying Out** vì mạng quá sâu thông tin bị mất dần khi lan truyền qua các hidden layer, tác giả đã sử dụng **2 auxiliary classifiers (Softmax)** để làm 2 output cho module \n",
    "            \n",
    "### The total loss used by the inception net during training.\n",
    "### Weight value used in the paper was 0.3 for each auxiliary loss.\n",
    "  **total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2**\n",
    "    \n",
    "#### Problem\n",
    "\n",
    "Trên thực tế gradients thường sẽ có giá trị nhỏ dần khi đi xuống các layer thấp hơn. Kết quả là các cập nhật thực hiện bởi Gradient Descent không làm thay đổi nhiều weights của các layer đó, khiến chúng không thể hội tụ và DNN sẽ không thu được kết quả tốt. Hiện tượng này được gọi là Vanishing Gradients.\n",
    "\n",
    "## Reference\n",
    "A Simple Guide to the Versions of the Inception Network\n",
    "https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------SqueezeNet-------------------------------\n",
    "\n",
    "## Introduction And Motivation\n",
    "    * Muốn đưa model vào các Mini-PC , smartphone, các FPGAs\n",
    "    * Thông thường các thiết bị chỉ sử dụng CPU, và cố bộ nhớ nhỏ, FPGA (10 MB)\n",
    "    * Model nhỏ, giảm lượng params trong model nhưng vẫn giữ được  Accuracy tương đương.\n",
    "\n",
    "## General\n",
    "    * SqueezeNet cung cấp một kiểu cấu trúc thông minh về những phân tích định lượng cho network.\n",
    "    * Có thể đạt được Alexnet's Accuracy, nhưng nhanh gấp 3 lần và nhỏ hơn 500 lần Alexnet\n",
    "    \n",
    "![title](Doc/sq.png)\n",
    "\n",
    "## ARCHITECTURAL DESIGN STRATEGIES\n",
    "\n",
    "    1. Vì cần làm nhỏ model, số lượng Conv cũng phải hạn chế , bằng cách sử dụng các filter size 1x1 để thay thế cho các filters 3x3 để giảm số params (lượng params 1x1 nhỏ hơn 9 lần 3x3 filter). \n",
    "    \n",
    "    2. Giảm số Input Channel to 3x3 filters(3x3) bằng cách sử dụng Squeeze layer. Xém xét 1 Conv1 là 3x3 filter. Tổng số params trong layer này là\n",
    "    \n",
    "    (number of input channels) * (number of filters) * (3*3)\n",
    "    \n",
    "    96 * 16 * 3 * 3 = 13824 params\n",
    "    \n",
    "    \n",
    "    \n",
    "    3. Thực hiện Downsampling (max pooling )\n",
    "       - Nếu các layer ở trước trong network, đặt downsampling với số stride lớn, thì hầu hết output của các layers là các activation maps nhỏ.\n",
    "       - Ngược lại, Các layer được đặt downsampling với stride 1 và lớn hơn 1 được tập trung vào cuối network thì sẽ ouput ra activation maps lớn. \n",
    "       - Và, Nếu có các large Feature maps (applied delayed downsampling) có thể đưa đến Accuracy cho Classification cao hơn so với các mạng khác tương đương. \n",
    "    \n",
    "    \n",
    "P/s Giống như mạng Inception thì Google đã sử dụng filter 1x1 để giảm dimension của feature trước khi đưa vào Conv layer lớn hơn (3x3, 5x5). Điều này giúp giảm chi phí tính toán cho mạng.\n",
    "\n",
    "\n",
    "## Fire Modules\n",
    "\n",
    "SqueezeNet sử dụng Fire Module thay cho Conv Layers. \n",
    "\n",
    "  - Với kiểu Arch này theo ý tưởng \n",
    "       * Dùng 1x1 filter để giảm sô Params ( idea 1)\n",
    "       * Thông qua 1x1 filter ngay ban đầu để giảm số Input Channel (idea 2)\n",
    "    \n",
    "## SqueezeNet Architecture\n",
    "\n",
    "![title](Doc/sqck1.png)\n",
    "\n",
    "\n",
    "![title](Doc/sqck2.png)\n",
    "\n",
    "* Có 2 layers: Squeeze layer và Expand layer.\n",
    "    - Sử dụng 2 Convolutional Layer (Conv1,Conv10)\n",
    "    - Xếp chồng các Fire Modules (Fire 2- 9) lên với nhau\n",
    "    - Max Pooing thì đặt vào sau các block conv1, fire4, fire8, conv10\n",
    "    - Squeeze layer và Expand layer giữ cùng Feature Map size.\n",
    "\n",
    "\n",
    "## Try to do \n",
    "\n",
    "* Fire Modules chưá 1 Squeece Layer 1x1 filter và  Expand layer (1x1 và 3x3 filter).\n",
    "* Nếu chúng ta lâys Fire2 thì Input của Fire2 sẽ là  55x55x96\n",
    "* Lấy 1x1 Filter với depth là 16 -> Return ouput là 55x55x16\n",
    "* Sau đó, 55x55x16 đưa vào 2 Conv là 64 1x1 và 64 3x3. Kết hợp 2 ouput sé là 55x55x128\n",
    "\n",
    "\n",
    "![title](Doc/sqck3.png)\n",
    "\n",
    "* Mỗi Fire Layer có layer nên, ý chỉ số depth = 2 (gồm  squeeze layer và theo sau là expand layer)\n",
    "\n",
    "\n",
    "** Param = ((n*m*l)+1)*k = ((3*3*l)+1)*32 = 18,464, With: **\n",
    "    . Input 28*28*64\n",
    "    . m=n=3 is the size of the kernel \n",
    "    . l=64 is the number of channels in the input\n",
    "    . k=32 is the number of filters\n",
    "    . The +1 is used to add the biases.\n",
    "     \n",
    "    \n",
    "### Thử tính lượng params tại fire2 trước khi prunning\n",
    "\n",
    "#### Tại Layer 1 - The Squeeze Layer - S(1x1)\n",
    "l = 96, m = n = 1 and k = 16\n",
    "\n",
    "**((n*m*l)+1)*k = ((1*1*96)+1)*16 = 97 * 16 = 1552\n",
    "\n",
    "#### Tại Layer 2 - The expansion layer\n",
    "\n",
    "##### Kernel size 1 - e1x1\n",
    "l=16, m=n=1 and k=64. \n",
    "\n",
    "**((n*m*l) + 1) * k = ((1*1*16)+1) * 64 = 1088\n",
    "\n",
    "##### Kernel size 3 - e3x3\n",
    "l=16, m=n=3 and k=64.\n",
    "\n",
    "**((n*m*l) + 1) * k = ((3*3*16)+1) * 64 = 9280\n",
    "\n",
    "#### Total\n",
    "\n",
    " **1,552+1,088+9,280 = 11,920\n",
    " \n",
    " \n",
    "## Other SqueezeNet Details\n",
    "\n",
    " \n",
    "\n",
    "## -------------- Example from Inception --------------------\n",
    "![title](Doc/googlen4.png)\n",
    "\n",
    "  ##### <u>Input 28x28x192 - Kernel 5x5 - Feature Maps 32x28x28:</u>\n",
    "  #### Với 5^2(28)^2(192)(32)=120,422,400 operations.\n",
    "\n",
    "\n",
    "![title](Doc/googlen5.png)\n",
    "\n",
    "Với kernel 1x1 đặt trước kernel 5x5 và output là 28x28x16 (tra tại bảng 1.1) \n",
    "![title](Doc/googlen6.png)\n",
    "\n",
    "Và chi phí tính toàn giảm khoảng 10 lần so với kiến trúc model tuyền thống.\n",
    "    \n",
    "## Sumup\n",
    "\n",
    "Với idea 1 và 2 là chiến lược giảm số params trong mạng, để có thể chi phí tính toán\n",
    "Idea 3 nhiệm vụ chính là giữ cho độ chính xác được bảo toàn.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "* Using Pytoch to implement SquezeeNet-None 1.0\n",
    "\n",
    "* Dataset: CiFAR10\n",
    "\n",
    "* Saved Model: 3.0 MB (3,018,880 bytes) (Alexnet size is 240MB)\n",
    "\n",
    "* P/s: - With Epoch 1~ 5, GPU Performance was 70~90%.  \n",
    "    \n",
    "       - But, when step over Epoch 6~55, GPU performance was sometimes for 5%, 50%, and 0% for all of the remain epoch \n",
    "       \n",
    "       - I didn't know why it had gone down GPU performance like that. \n",
    "\n",
    "![title](Doc/Net_loss.jpg)\n",
    "\n",
    "\n",
    "![title](Doc/sqacc.png)\n",
    "\n",
    "    \n",
    "## Reference\n",
    "SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH\n",
    "50X FEWER PARAMETERS AND <0.5MB MODEL SIZE\n",
    "\n",
    "https://arxiv.org/pdf/1602.07360.pdf\n",
    "\n",
    "Notes on SqueezeNet\n",
    "\n",
    "https://medium.com/@smallfishbigsea/notes-of-squeezenet-4137d51feef4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------- Prunning Convolutional NN ------------\n",
    "\n",
    "## ASTRACT \n",
    "\n",
    "* Tác giả xem kẻ việc criteria-based prunning (cắt tỉa dựa trên tiêu chí) với finetuning by Backpropagation\n",
    "    - Đảm bảo việc tính toán hiệu quả, dẫn đến Good generalization (Tính tổng hóa) trong mạng đã prune\n",
    "    \n",
    "* Tác giả giới thiệu 1 tiêu chí mới dựa trên Taylor Expansion (TE). TE \n",
    "\n",
    "* Tác giả focus vào Transfer learning, bởi vì các mạng có  pretrained lớn đáp ứng cho các công việc đặc biệt.\n",
    "\n",
    "* Tác giả đã chứng minh ý tưởng của mình bằng việc pruning VGG16, speedup x3, downsize x4\n",
    "\n",
    "## Problem\n",
    "\n",
    "- Nhúng các Deep Learning models vào các thiết bị sử dụng CPU và memory nhỏ như Smartphone, Camere giám sát, FPGA, mini-pc  \n",
    "    \n",
    "## General\n",
    "\n",
    "- Pruning các filter của CNN, làm nhẹ model. Bằng idead là trong nhiều params trong network, sẽ có một số cái dư thừa và không đóng góp gì đến ouput\n",
    "\n",
    "- Với đặc điểm này, nếu đánh rank chỉ số contribute của các Nerons. Điều này  có thể remove đi các Low Ranking Nerons, làm cho Network smaller and faster\n",
    "\n",
    "- Sau khi pruning Accuracy sẽ bị drop và network phải tiếp tục trained nhiều hơn để phục hồi lại Accs.\n",
    "    \n",
    "    P/s: Nếu pruning một lúc prune quá nhiều, network sẽ bị phá hủy, không thể phục hồi lại dc \n",
    "\n",
    "- Có rất nhiều papers về pruning, nhưng một vài lí do sau đây làm cho nó không hoạt động tốt:\n",
    "\n",
    "    - Việc ranking không đủ tốt ---> Accuracy is dropped strongly (T.T)\n",
    "    - Implement khó khăn\n",
    "    - Những người sử dụng kỹ thuật pruning, không chia sẽ, giữ lại như một bí mật\n",
    "\n",
    "## How to prune\n",
    "\n",
    "- Ranking có thể triển khai theo L1/L2 mean của Neron Weights, Mean Activation của các nerons, sô lần neuron được tính về zero trong khi Valuaion....\n",
    "\n",
    "## Flow for Pruning Model\n",
    "\n",
    "** Iterative Pruning: Prune/Train/Repeat\n",
    "\n",
    "\n",
    "\n",
    "![title](Doc/pruning_steps.png)\n",
    "\n",
    "## Where is pruning in Network?\n",
    "\n",
    "- Trong VGG16:\n",
    "    - 90% weights nằm trong Fully Connected layer (FC), 1% chiếm tổng số điểm đó hoạt động.\n",
    "    - Các công việc hầu hết tập trung vào phần FC, cắt tĩa nhưng điểm dư thừa, model size cũng có thể giảm đáng kể\n",
    "\n",
    "- Tác giả cũng sẽ tập trung vào việc prune các filter tại CONV Layers.\n",
    "\n",
    "- Nhưng nó gây ảnh hưởng tới việc giảm memory, có nghĩa là lúc lớp CONV cuối cùng sẽ bị prune quá nhiều và dồng thời các neuron tại FC kế đó cũng phải bik bỏ đi.\n",
    "\n",
    "- Khi pruning Conv Filter, hoặc là giảm weights trong mỗi filter, hoặc là remove 1 độ lớn (a specific dimension) cụ thể nào đó của 1 Kernel. Khi filter trở nên thưa thứa, nó không giúp cho việc tính toán nhanh hơn. \n",
    "\n",
    "**P/s: Một điều khá quang trọng trong bài báo đề cập đến là \"Đầu tiên là training , sau đó pruning một Larger Network, đặc biệt là trong Transfer Learning, Chúng đem lại kết quả tốt hơn nhìu so với các mạng nhỏ được cái mà được training từ đầu\"**\n",
    "\n",
    "   From Source: \"is that by training and then pruning a larger network, especially in the case of transfer learning, they get results that are much better than training a smaller network from scratch.\"\n",
    "   \n",
    "## Briefly review \n",
    "\n",
    "### Pruning filters for effecient convnets\n",
    "\n",
    "Paper: https://arxiv.org/abs/1608.08710\n",
    "\n",
    "![title](Doc/prune1.png)   \n",
    "\n",
    "Trong bài báo này, tác giả đã pruning các CONV filters \n",
    "\n",
    "- Pruning các filter không quan trọng trên bằng cách tính tổng weight L1-norm.\n",
    "\n",
    "- Các Filters với weights của Kernel nhở hơn, có nghĩa việc tạo ra các feature maps bằng các giá trị activation yếu. Nó sẽ được đem đi so sánh với các cái activation node khác. \n",
    "\n",
    "- Tác giả thấy rằng việc pruning các weight kernel nhỏ nhất mang lại hiệu quả tốt hơn khi pruning một cách random or các filter lớn nhất\n",
    "\n",
    "- Pruning một filter với index k, ảnh hưởng đến các layer mà nó nằm trông đấy.\n",
    "\n",
    "- Tất cả các input channels tại index k của lớp phía sau sẽ bị remove hoàn toàn. Nó sẽ không còn còn tại khi bị pruning nửa.\n",
    "\n",
    "- Trong trường hợp lớp phía sau là FC layer, kích thước Feature Map của kênh đó là MxN, thì  MxN neurons tại FC sẽ bị remove tương tương.\n",
    "\n",
    "- Việc Ranking cũng đơn giản, chỉ cần L1 Norm cho Weights trên mỗi filter. Mỗi lần pruning chúng ta rank tất cả filter, lấy khoảng m ranking filter thấp nhất sẽ bị pruned trên toàn tất cả layer, retrain và repeat.\n",
    "\n",
    "### Structured Pruning of Deep Convolutional Neural Networks\n",
    "\n",
    "Paper: https://arxiv.org/abs/1512.08571\n",
    "\n",
    "Trong bài báo này, cách làm cũng tương tự\n",
    "\n",
    "\n",
    "\n",
    "- Nhưng, việc ranking xử lí phức tạp hơn nhiều. \n",
    "    \n",
    "    * 1 tập hợp N filters, cái sẽ đại diện cho N CONV filters bị pruned\n",
    "    \n",
    "    * Each particle is assigned a score based on the network accuracy on a validation set, when the filter represented by the particle was not masked out. Then based on the new score, new pruning masks are sampled. (Không hiểu)\n",
    "    \n",
    "    * Việc running process này khá nặng, nên tác giả đã xem xét triển khai tính toán số điểm với tập validation nhỏ.\n",
    "    \n",
    "###  Pruning Convolutional Neural Networks for Resource Efficient Inference (*)\n",
    "\n",
    "Đầu tiên, họ coi vấn đề pruning là bài toán tối ưu hóa tổng hợp:\n",
    "\n",
    "![title](Doc/prune2.png)  \n",
    "\n",
    "- Chọn 1 tập con của weights B như trên hình, như vậy khi pruning sẽ giảm được Cost Change ở mức thấp nhất.\n",
    "\n",
    "- Sử dụng giá trị tuyệt đối ở đây, phải triển khai sao cho pruned network sẽ không giảm preformance quá nhiều. \n",
    "\n",
    "## Oracle pruning\n",
    "\n",
    "\n",
    "![title](Doc/taylor1.png) \n",
    "\n",
    "![title](Doc/taylor2.png) \n",
    "\n",
    "> C(W, D) is the average network cost function on the dataset D\n",
    "> Weights are set to W\n",
    "> Evaluate C(W, D) as an expansion around C(W, D, h = 0)\n",
    "\n",
    "- VGG16 có 4224 Conv filters. \n",
    "\n",
    "- Để đánh ranking, tác gỉa dùng phương phấp brute-force-prune (Duyệt Trâu - Duyệt từ a đến z) trên mỗi filter.\n",
    "\n",
    "- Sau đó, quan sát sự thay đổi của Cost Function trong khi training. \n",
    "\n",
    "- Họ cũng đưa ra một phương pháp để đánh ranking neuron mới dựa trên Taylor Expension (tính toán khá nhanh ) cho Network Cost Function\n",
    "\n",
    "**P/s: Prunning một filter h giống như là đư filter đó về 0**\n",
    "**     Việc đánh giá C(W, D) và C(W, D, h = 0) nên gần bằng nhau, chứng minh việc prune 1 số filter không làm ảnh hưởng đến cost quá nhiều**\n",
    "\n",
    "- Còn việc đánh ranking bằng L2 norm, thì được cho là dựa theo kinh nghiệm. Nhưng nó đem lại chất lượng khá tốt khi pruning\n",
    "\n",
    "- Tác giả đã dùng cả activation và gradient như là Ranking Methods. Nếu bất cứ cái nào có high values, có nghĩa nó có contributed đến đầu ra.\n",
    "\n",
    "## Step By Step To Pruning\n",
    "\n",
    "### Step One - Train A Large Network\n",
    "\n",
    "- Sử dụng VGG-16, drop đi lớp FC cũ, and thêm 3 lớp FC mới. \n",
    "- Sau đó, Freeze tất cả Conv Layers, \n",
    "- Retrain chỉ 3 lớp FC mới\n",
    "\n",
    "```\n",
    "    self.classifier = nn.Sequential(\n",
    "\t    nn.Dropout(),\n",
    "\t    nn.Linear(25088, 4096),\n",
    "\t    nn.ReLU(inplace=True),\n",
    "\t    nn.Dropout(),\n",
    "\t    nn.Linear(4096, 4096),\n",
    "\t    nn.ReLU(inplace=True),\n",
    "\t    nn.Linear(4096, 2))\n",
    "        \n",
    "```\n",
    "-> Sau khi training khoảng 20 epochs thì đạt được độ chính xác 98.7% trên test set\n",
    "\n",
    "### Step Two - Rank The Filters\n",
    "\n",
    "- Để tính toán Taylor Criteria, cần thực hiện Forward + Backward  trên tập dữ liệu của mình. Hoặc 1 phần nhỏ nếu data quá lớn.\n",
    "\n",
    "- Tiếp theo, bằng cách nào để  lấy được cả Gradient và Activation for Conv Layers. Trong Pytorch, có thể register a hook trên Gradient Computation, \n",
    "\n",
    "```\n",
    "# Get Activation\n",
    "\n",
    "for layer, (name, module) in    enumerate(self.model.features._modules.items()):\n",
    "\tx = module(x)\n",
    "\tif isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "\t\tx.register_hook(self.compute_rank)\n",
    "\t\tself.activations.append(x)\n",
    "\t\tself.activation_to_layer[activation_index] = layer\n",
    "\t\tactivation_index += 1\n",
    "```\n",
    "\n",
    "\n",
    "- Bây giờ, đã có activation in self.activation, Gradient cũng sẵn sàng, tính toán Rank\n",
    "\n",
    "```\n",
    "def compute_rank(self, grad):\n",
    "\tactivation_index = len(self.activations) - self.grad_index - 1\n",
    "\tactivation = self.activations[activation_index]\n",
    "\tvalues = \\\n",
    "\t\ttorch.sum((activation * grad), dim = 0).\\\n",
    "\t\t\tsum(dim=2).sum(dim=3)[0, :, 0, 0].data\n",
    "\t\n",
    "\t# Normalize the rank by the filter dimensions\n",
    "\tvalues = \\\n",
    "\t\tvalues / (activation.size(0) * activation.size(2) * activation.size(3))\n",
    "\n",
    "\tif activation_index not in self.filter_ranks:\n",
    "\t\tself.filter_ranks[activation_index] = \\\n",
    "\t\t\ttorch.FloatTensor(activation.size(1)).zero_().cuda()\n",
    "\n",
    "\tself.filter_ranks[activation_index] += values\n",
    "\tself.grad_index += 1\n",
    "```\n",
    "\n",
    "**For example, if the batch size was 32, the number of outputs for a specific activation was 256 and the spatial size of that activation was 112x112 such the activation/gradient shapes were 32x256x112x112, then the output will be a 256 sized vector representing the ranks of the 256 filters in this layer.**\n",
    "\n",
    "- N là số tập hợp các ranking filter thấp nhất. Sử dụng N=512. Chúng ta sẽ remove 12% from 4224 Conv Filters.\n",
    "\n",
    "### Step 3 - Fine tune and repeat\n",
    "\n",
    "- Ở giai đoạn này, bỏ freeze tất cả layers và retain Network for 10 epochs, đủ để đạt được kế quả tốt từ dataset\n",
    "\n",
    "- Sau back về Step 1 với mạng đã được thay đổi và lặp lại\n",
    "\n",
    "## Experiment\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Dataset: Dogs and Cats Datasets from the Kaggle, with \n",
    "    * Train\n",
    "\n",
    "            ......... dogs -> 1000 Images\n",
    "\n",
    "            ......... cats -> 1000 Images\n",
    "\n",
    "    * Test\n",
    "\n",
    "            ......... dogs -> 400 Images\n",
    "\n",
    "            ......... cats -> 400 Images\n",
    "           \n",
    "- After Prunning is model as:    \n",
    "    * train_model size is **537.1 MB (537,102,059 bytes)**\n",
    "    * prunned_model size is **148.1 MB (148,140,795 bytes)**\n",
    "    \n",
    "    \n",
    "- Time to infer\n",
    "    * train_model is 4.550 sec\n",
    "    * prunned_model is 2.30122 sec\n",
    "    \n",
    "    \n",
    "- Total of the Filter in Network\n",
    "    * train_model is 4224 filters\n",
    "    * prunned_model is 1664 filters\n",
    "\n",
    "### Step 1: Build a new Classifier with own dataset\n",
    "\n",
    "![title](Doc/prune3.png)\n",
    "\n",
    "### Step 2: Load trained model from step 1 to prune\n",
    "\n",
    "* With 10 epoch for each iteration (has 3 iters) for prunning and trained more and more, to recover Accuracy after removing unimportant filters\n",
    "\n",
    "![title](Doc/prune4.png)\n",
    "![title](Doc/prune5.png)\n",
    "\n",
    "### Step 3: Test both official pretrained model and prunned model\n",
    "\n",
    "    #### Official Pretrain Model\n",
    "![title](Doc/prune7.png)\n",
    "\n",
    "    #### Prunned Model\n",
    "![title](Doc/prune6.png)\n",
    "\n",
    "\n",
    "\n",
    "## Reference\n",
    "Pruning deep neural networks to make them fast and small\n",
    "\n",
    "https://jacobgil.github.io/deeplearning/pruning-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n",
    "\n",
    "## ------------------------------------Featute maps (Activation Maps): \n",
    "\n",
    "https://computersciencewiki.org/index.php/Feature_maps_(Activation_maps)\n",
    "\n",
    "![title](Doc/featurem.png)\n",
    "\n",
    "    Đây được coi như là một tập hợp của các đặc trưng bằng việc detect được từ các kernel (tại Conv layer) như đường cong, edge... \n",
    "<u>Ví dụ:</u> Tại Conv1 chúng ta thu được 1 set các đường cong, thẳng... của một chiếc xe đạp. kết hợp với set các cạnh, góc rõ ràng hơn như nửa bánh xe, bàn đạp... để có thể dự đoán đó là chiếc xe đạp.\n",
    "\n",
    "## ------------------------------------Vanishing / Exploding Gradient Problems\n",
    "   \n",
    "   Backpropagation Algorithm sẽ **tính toán** gradient của Cost Functions tương ứng với từng weights của network, từ output ngược trở về lại input.\n",
    "    Sau đó, Gradient Descent sẽ được dùng để cập nhật các weights params cho đến khi params của network hội tụ. Nhưng,\n",
    "    \n",
    "      * Nếu số lượng vòng lặp quá nhỏ -> Bad result\n",
    "      * Nếu số lượng vòng lặp quá lớn -> Take long time for training\n",
    "   \n",
    "   Thực tế, qua quá trình thực địa, gradient thường sẽ có giá trị nhỏ dần (gần như bằng zero) khi đi xuống các layer thấp hơn. Điều này gây ra việc Gradient Descent không làm thay đổi quá nhiều weights của các layer và không thể hội tụ. Mạng sẽ không cho ra kết quả tốt.\n",
    "   \n",
    "   ----> **Vanishing Gradients.**\n",
    "   \n",
    "   Trong nhiều trường hợp khác, gradients có thể có giá trị lớn hơn trong quá trình backpropagation, khiến một số layers có giá trị cập nhật cho weights quá lớn khiến chúng phân kỳ (phân rã), tất nhiên DNN cũng sẽ không có kết quả như mong muốn. Hiện tượng này được gọi là Exploding Gradients, và thường gặp khi sử dụng Recurrent Neural Networks (RNNs).\n",
    "   -----> **Exploding Gradients.**\n",
    "   \n",
    "   ### How to prevent Vanishing / Exploding Gradient Problems\n",
    "* Xavier and He Initialization Techniques (X)\n",
    "* Nonsaturating Activation Functions (X)\n",
    "* Gradient Clipping (X)\n",
    "* Batch Normalization (Overview)\n",
    "\n",
    "## ------------------------------------Batch Normalization\n",
    "      \n",
    "### 1. Why do use Normalization?\n",
    "   \n",
    "         - Chúng ta điều chỉnh và chia tỉ lệ cho các hàm kích hoạt. Feature thì từ 0-1 và cũng sẽ có các feature từ 1-1000. Vì vậy, phải chuẩn hóa (normalize) chúng để tăng tốc việc học (Learning).\n",
    "         - Thay vì chỉ normalize các input đến Network, Việc chuẩn hóa cho các inputs đến hidden layer giúp việc tăng tốc và cải thiện cho tốc độ training lên gấp 10 lần.\n",
    "         \n",
    "### 2. Batch Normalization (BN) is helpful\n",
    "      \n",
    "          - Giúp có thể sử dụng learning rate cao hơn tằng tốc độ cho Network Train, bởi vì BN có  đã chuẩn hóa các batch data đầu vào mà không có cái nào quá cao or quá thấp. Gradient Decent thường đòi hỏi tỉ lệ học tập nhỏ để mạng hội tụ. Và khi mạng sâu hơn, cần nhiều lần lặp hơn để trong lúc Backpagation tính toán gradient, vì lúc này gradient sẽ nhỏ hơn qua các lần tình toán.\n",
    "          \n",
    "          - Weight Initialization là vấn đề trong Network Train và là vấn đề lớn khi chúng ta có một Deeper Network. Tuy nhiên, BN sẽ quan tâm về việc chọn Init Weight từ đâu \n",
    "          - Giúp cho các activation funtions hiệu quả hơn. Sigmoid làm Gradient chúng ta  mất giá trị rất nhanh, điều này dẫn đến không thể dùng cho các mạng Deep. Còn ReLu function, thường \"Die Out\" khi training, gây ra việc dừng training hoàn toàn, do đó phải xem xét cẩn thận cho khoảng giá trị nạp vào ReLU Funcs. May mắn là BN sẽ giúp ta điều chỉnh các values này ứng với mỗi Activate Funcs\n",
    "          \n",
    "          - Đơn giản hóa việc build một Deeper Network. Ba yếu tố trên đã được cải thiện. Vì vậy, sẽ dễ dàng build và train deeper NN nhanh hơn khi có BN. \n",
    "<u>Trích:</u> \"And it’s been shown that deeper networks generally produce better results, so that’s great.\"\n",
    "             \n",
    "          - Giảm overfitting bởi vì BN cung cấp 1 chút Regularization, nó giống với Dropout, nó thêm noise đến Network. Trong khi đối với Inception Modules, BN và Dropout làm việc tốt như nhau. tuy nhiên về tổng thể thì BN được xem xét như là 1 phần nhỏ của Regulariation. \n",
    "          \n",
    "          - Điều này sẽ giúp chỉ cần sử dụng 1 vài dropout, nó tốt hơn vì dùng dropout quá nhiều sẽ làm chúng ta mất nhiều thông tin.\n",
    "          \n",
    "          - Cho ra kết quả tốt hơn. Tại sao, khi tốc độ train nhanh hơn, có nghĩa có thể lặp lại xây dựng nhiều kiểu design một cách nhanh hơn. Điều này giúp chúng ta xây dựng một mạng Deeper Network.\n",
    "          \n",
    "   **P/s: nó không có nghĩa là phụ thuộc hoàn toàn vào Batch Normalization for Regularization. Nên kết hợp cả BN với Dropout**\n",
    "   \n",
    "### 3. How does batch normalization work?\n",
    "       \n",
    "           - Để tăng sự ổn định cho Neural Network, BN nó thêm một công  đoạn chuẩn hóa và zero-centering(mean subtracting) các inputs (mean của inputs sẽ là 0) đặt vào trước activation layer. \n",
    "           \n",
    "           - Kết quả sau đó sẽ được scaling và shifting sử dụng hai parameters cho mỗi layer. Để thực hiện normalizing và zero-centering, Batch-Norm sẽ tính độ lệch chuẩn và phương sai của các inputs trên các mini-batches, sau đó sử dụng hai parameter là γ và β để thực hiện việc scaling.\n",
    "           \n",
    "           \n",
    "   <u>**Sumup for Batch Normalization**</u>\n",
    "   \n",
    "      * BN tối ưu cho Network Training:\n",
    "          - Có thể sử dụng learning rate lớn để tăng tốc thời gian training.\n",
    "          - Giảm thiệu sự phụ thuộc vào quá trính khởi tạo weight \n",
    "          - Giúp cho các activation funtions hiệu quả hơn\n",
    "          - Đơn giản hóa việc build một Deeper Network\n",
    "          - Giảm đến mức tối thiểu hiện tượng Vanishing/Exploding Gradients. \n",
    "          - Tránh overfitting và giảm số lượng Dropout sẽ được sủ dụng trong network.\n",
    "          - Batch - Norm có thẻ được sử dụng như một regularizer giúp giảm thiểu Overfitting\n",
    "          - Điều chúng ta muốn là gì: \"Cho ra kết quả Perfect nhất!!! :)) \"\n",
    "          \n",
    "  **P/s: Nhược điểm là sẽ làm model trở  nên phức tạp hơn trong việc tính toán, tốn nhiều thời gian khi predictions.**\n",
    "  \n",
    " \n",
    "# Batch Normalization By Chainer\n",
    "\n",
    "**x = np.arange(12).reshape(4, 3).astype(np.float32) ** 2\n",
    " \n",
    "\n",
    "\n",
    "array([[  0.,   1.,   4.],\n",
    "      [  9.,  16.,  25.],\n",
    "      [ 36.,  49.,  64.],\n",
    "      [ 81., 100., 121.]], dtype=float32)\n",
    "\n",
    "\n",
    "**bn = chainer.links.BatchNormalization(3)**\n",
    "\n",
    "\n",
    "\n",
    "**bn(x):**\n",
    "\n",
    "variable([[-1.        , -1.0664359 , -1.1117983 ],\n",
    "          [-0.71428573, -0.6714596 , -0.6401263 ],\n",
    "          [ 0.14285715,  0.19748813,  0.23583598],\n",
    "          [ 1.5714287 ,  1.5404074 ,  1.5160885 ]])\n",
    "\n",
    "          \n",
    "\n",
    "## Try to do by \n",
    "\n",
    "\n",
    "![title](Doc/bn.png)\n",
    "\n",
    "\n",
    "\n",
    "** bn(x )= (x - x.mean(axis=0)) / np.sqrt(x.var(axis=0) + 2e-5)**\n",
    "\n",
    "\n",
    "array([[-1.        , -1.0664359 , -1.1117983 ],\n",
    "\n",
    "       [-0.71428573, -0.6714596 , -0.6401263 ],\n",
    "       \n",
    "       [ 0.14285715,  0.19748813,  0.235836  ],\n",
    "       \n",
    "       [ 1.5714285 ,  1.5404074 ,  1.5160886 ]], dtype=float32)\n",
    "\n",
    "\n",
    "\n",
    "## Batch Normarlization -- Before or After ReLU?\n",
    "\n",
    "**Note, that results are obtained without mentioned in paper y=kx+b additional layer.\n",
    "\n",
    "![title](Doc/wbnp.png)\n",
    "\n",
    "Vấn đề đặt BN ở đâu hiện tại vẫn đang được tranh luận, hình trên được trích từ:\n",
    "https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\n",
    "\n",
    "## -------------------------------------- Bottleneck Layer\n",
    "![title](Doc/bnck.png)\n",
    "\n",
    "### What is the Bottleneck Layer?\n",
    "\n",
    "    * Trong NN, bottlneck chỉ là 1 layer với số lượng neurons ít hơn các layer dưới và trên nó. \n",
    "    \n",
    "    * Chức năng là để nén các Feature Representations (Bộ biểu diễn các đặc trưng ảnh), giúp cho việc tính toán Loss tốt nhất khi training\n",
    "    \n",
    "### Using in Inception\n",
    "\n",
    "    * Nó sẽ làm giảm số features, và do đó trên mỗi layer, thời gian inference (Prediction) sẽ được giữ ở mức thấp, trước khi đưa data đến các Conv Modules (số lượng Features giảm 4 lần theo tác giả đã trình bày).\n",
    "    \n",
    "    * Điều này đưa đến thành công cho GoogleNet trước và sau này với computational cost ít\n",
    "    \n",
    "    * Một điêu đem đến  thế mạnh cho Bottleneck layer là Input Features có tính tương quan. Có nghĩa là, nó sẽ loại bỏ sự thừa thải bằng cách kết hợp với 1x1 Conv. Sau đó, các Conv phía sau có số lượng features nhỏ hơn, và chúng lại có thể mở rộng dimension như lúc ban đầu để đưa vào các lớp tiếp theo.\n",
    "    \n",
    "<u>Example:</u>\n",
    "         \n",
    "256 features coming in, 256 coming out\n",
    "         \n",
    "   Case 1: Inception with only 3x3 Conv\n",
    "                 \n",
    "                * 256x256 x 3x3 = 589,000s multiply-accumulate\n",
    "                \n",
    "   Case 2: Perform 256 to 64 by using 1x1 Conv, then 64 convolution on all Inception branches, and then we use again a 1x1 convolution from 64 -> 256 features back again.\n",
    "   \n",
    "                * 256×64 × 1×1 = 16,000s\n",
    "                * 64×64 × 3×3 = 36,000s\n",
    "                * 64×256 × 1×1 = 16,000s\n",
    "                \n",
    "                \n",
    "                \n",
    "## -------------------------------------- Class Activation Mapping\n",
    "\n",
    "\n",
    "\n",
    "![title](Doc/ac.jpg)\n",
    "\n",
    "![title](Doc/ac1.jpg)\n",
    "\n",
    "\n",
    "## ------------------------ L1/L2 mean\n",
    "\n",
    "    \n",
    "   \n",
    "# Reference\n",
    "1. Batch normalization in Neural Networks\n",
    " \n",
    "https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "\n",
    "2. Batch Normalization\n",
    "\n",
    "https://towardsdatascience.com/batch-normalization-8a2e585775c9\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL BY CHAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source Code: https://qiita.com/icoxfog417/items/5aa1b3f87bb294f84bac\n",
    "class Alex(chainer.Chain):\n",
    "\n",
    "    insize = 227\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Alex, self).__init__(\n",
    "            conv1=L.Convolution2D(3,  96, 11, stride=4),\n",
    "            conv2=L.Convolution2D(96, 256,  5, pad=2),\n",
    "            conv3=L.Convolution2D(256, 384,  3, pad=1),\n",
    "            conv4=L.Convolution2D(384, 384,  3, pad=1),\n",
    "            conv5=L.Convolution2D(384, 256,  3, pad=1),\n",
    "            fc6=L.Linear(9216, 4096),\n",
    "            fc7=L.Linear(4096, 4096),\n",
    "            fc8=L.Linear(4096, 1000),\n",
    "        )\n",
    "        self.train = True\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        self.clear()\n",
    "        h = F.max_pooling_2d(F.relu(\n",
    "            F.local_response_normalization(self.conv1(x))), 3, stride=2)\n",
    "        h = F.max_pooling_2d(F.relu(\n",
    "            F.local_response_normalization(self.conv2(h))), 3, stride=2)\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)\n",
    "        h = F.dropout(F.relu(self.fc6(h)), train=self.train)\n",
    "        h = F.dropout(F.relu(self.fc7(h)), train=self.train)\n",
    "        h = self.fc8(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source Code: http://nocotan.github.io/chainer/2017/08/04/chainercnn-copy.html\n",
    "class VGG16(chainer.Chain):\n",
    "    def __init__(self, num_class, train=True):\n",
    "        super(VGG16Net, self).__init__()\n",
    "        with self.init_scope():\n",
    "            '''\n",
    "                Init Conv1 with:\n",
    "                    Input: None\n",
    "                    output: 64 \n",
    "                    Kernel: 3x3\n",
    "                    Stride: 1\n",
    "                    Padding: 1 ( Spatial padding width for input arrays)\n",
    "            ''' \n",
    "             '''\n",
    "                Init Conv2 with:\n",
    "                    Input: 64\n",
    "                    output: 64\n",
    "                    Kernel: 3x3\n",
    "                    Stride: 1\n",
    "                    Padding: 1 ( Spatial padding width for input arrays)\n",
    "            ''' \n",
    "            self.conv1=L.Convolution2D(None, 64, 3, stride=1, pad=1)\n",
    "            self.conv2=L.Convolution2D(None, 64, 3, stride=1, pad=1)\n",
    "\n",
    "            self.conv3=L.Convolution2D(None, 128, 3, stride=1, pad=1)\n",
    "            self.conv4=L.Convolution2D(None, 128, 3, stride=1, pad=1)\n",
    "\n",
    "            self.conv5=L.Convolution2D(None, 256, 3, stride=1, pad=1)\n",
    "            self.conv6=L.Convolution2D(None, 256, 3, stride=1, pad=1)\n",
    "            self.conv7=L.Convolution2D(None, 256, 3, stride=1, pad=1)\n",
    "\n",
    "            self.conv8=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "            self.conv9=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "            self.conv10=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "\n",
    "            self.conv11=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "            self.conv12=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "            self.conv13=L.Convolution2D(None, 512, 3, stride=1, pad=1)\n",
    "\n",
    "            self.fc14=L.Linear(None, 4096)\n",
    "            self.fc15=L.Linear(None, 4096)\n",
    "            self.fc16=L.Linear(None, num_class)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "            Max Pooling size is 2x2\n",
    "            \n",
    "        '''\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.relu(self.conv2(h))), 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.relu(self.conv4(h))), 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv5(h))\n",
    "        h = F.relu(self.conv6(h))\n",
    "        h = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.relu(self.conv7(h))), 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv8(h))\n",
    "        h = F.relu(self.conv9(h))\n",
    "        h = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.relu(self.conv10(h))), 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv11(h))\n",
    "        h = F.relu(self.conv12(h))\n",
    "        h = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.relu(self.conv13(h))), 2, stride=2)\n",
    "\n",
    "        h = F.dropout(F.relu(self.fc14(h)))\n",
    "        h = F.dropout(F.relu(self.fc15(h)))\n",
    "        h = self.fc16(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50 -101 -152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source Code: https://github.com/mitmul/chainer-cifar10\n",
    "\n",
    "class BottleNeck(chainer.Chain):\n",
    "    def __init__(self, n_in, n_mid, n_out, stride=1, use_conv=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(BottleNeck, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(n_in, n_mid, kernel=1, stride, 0, True, w)\n",
    "            self.bn1 = L.BatchNormalization(n_mid)\n",
    "            self.conv2 = L.Convolution2D(n_mid, n_mid, 3, 1, 1, True, w)\n",
    "            self.bn2 = L.BatchNormalization(n_mid)\n",
    "            self.conv3 = L.Convolution2D(n_mid, n_out, 1, 1, 0, True, w)\n",
    "            self.bn3 = L.BatchNormalization(n_out)\n",
    "            if use_conv:\n",
    "                self.conv4 = L.Convolution2D(\n",
    "                    n_in, n_out, 1, stride, 0, True, w)\n",
    "                self.bn4 = L.BatchNormalization(n_out)\n",
    "        self.use_conv = use_conv\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn1(self.conv1(x)))\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = self.bn3(self.conv3(h))\n",
    "        return h + self.bn4(self.conv4(x)) if self.use_conv else h + x\n",
    "\n",
    "class Block(chainer.ChainList):\n",
    "    def __init__(self, n_in, n_mid, n_out, n_bottlenecks, stride=2):\n",
    "        super(Block, self).__init__()\n",
    "        self.add_link(BottleNeck(n_in, n_mid, n_out, stride, True))\n",
    "        for _ in range(n_bottlenecks - 1):\n",
    "            self.add_link(BottleNeck(n_out, n_mid, n_out))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self:\n",
    "            x = f(x)\n",
    "        return x\n",
    "class ResNet(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_class=10, n_blocks=[3, 4, 6, 3]):\n",
    "        super(ResNet, self).__init__()\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 64, 3, 1, 0, True, w)\n",
    "            self.bn2 = L.BatchNormalization(64)\n",
    "            self.res3 = Block(64, 64, 256, n_blocks[0], 1)\n",
    "            self.res4 = Block(256, 128, 512, n_blocks[1], 2)\n",
    "            self.res5 = Block(512, 256, 1024, n_blocks[2], 2)\n",
    "            self.res6 = Block(1024, 512, 2048, n_blocks[3], 2)\n",
    "            self.fc7 = L.Linear(None, n_class)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn2(self.conv1(x)))\n",
    "        h = self.res3(h)\n",
    "        h = self.res4(h)\n",
    "        h = self.res5(h)\n",
    "        h = self.res6(h)\n",
    "        h = F.average_pooling_2d(h, h.shape[2:])\n",
    "        h = self.fc7(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class ResNet50(ResNet):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(ResNet50, self).__init__(n_class, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "class ResNet101(ResNet):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(ResNet101, self).__init__(n_class, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "class ResNet152(ResNet):\n",
    "\n",
    "    def __init__(self, n_class=10):\n",
    "        super(ResNet152, self).__init__(n_class, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "# How to call model\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
    "    model = ResNet(10)\n",
    "    y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inception-V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source Code: https://github.com/delta2323/chainer-deepmark/blob/master/deepmark_chainer/net/inception_v3.py\n",
    "\n",
    "from chainer.functions.array import concat\n",
    "from chainer.functions.loss import softmax_cross_entropy as S\n",
    "from chainer.functions.noise import dropout\n",
    "from chainer.functions.pooling import average_pooling_2d as A\n",
    "from chainer.functions.pooling import max_pooling_2d as M\n",
    "from chainer import link\n",
    "from chainer.links.connection import convolution_2d as C\n",
    "from chainer.links.connection import linear\n",
    "from chainer.links.normalization import batch_normalization as B\n",
    "\n",
    "\n",
    "class AuxConv(link.Chain):\n",
    "\n",
    "    def __init__(self, conv, batch_norm=True, pool=None):\n",
    "        super(AuxConv, self).__init__(conv=conv)\n",
    "        if batch_norm:\n",
    "            out_channel = conv.W.data.shape[0]\n",
    "            self.add_link('batch_norm',\n",
    "                          B.BatchNormalization(out_channel))\n",
    "        self.pool = pool\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x, test=not train)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Sequential(link.ChainList):\n",
    "\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for l in self:\n",
    "            x = l(x, *args, **kwargs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Inception(link.ChainList):\n",
    "\n",
    "    def __init__(self, *links, **kw):\n",
    "        super(Inception, self).__init__(*links)\n",
    "        self.pool = kw.get('pool', None)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        xs = [l(x, train) for l in self]\n",
    "        if self.pool:\n",
    "            xs.append(self.pool(x))\n",
    "        return concat.concat(xs)\n",
    "\n",
    "\n",
    "class InceptionV3(link.Chain):\n",
    "    \"\"\"Inception V3.\n",
    "    http://arxiv.org/abs/1512.00567\n",
    "    https://github.com/tensorflow/models/blob/master/inception/inception/slim/inception_model.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_cudnn=True):\n",
    "        convolution = link.ChainList(\n",
    "            AuxConv(C.Convolution2D(3, 32, 3, 2, use_cudnn=use_cudnn)),\n",
    "            AuxConv(C.Convolution2D(32, 32, 3, use_cudnn=use_cudnn)),\n",
    "            AuxConv(C.Convolution2D(32, 64, 3, 1, 1, use_cudnn=use_cudnn)),\n",
    "            AuxConv(C.Convolution2D(64, 80, 3, 1, 1, use_cudnn=use_cudnn)),\n",
    "            AuxConv(C.Convolution2D(80, 192, 3, use_cudnn=use_cudnn)))\n",
    "\n",
    "        def inception_0(input_channel, pool_channel):\n",
    "            # 1x1\n",
    "            s1 = AuxConv(C.Convolution2D(input_channel, 64, 1, use_cudnn=use_cudnn))\n",
    "\n",
    "            # 5x5\n",
    "            s21 = AuxConv(C.Convolution2D(input_channel, 48, 1, use_cudnn=use_cudnn))\n",
    "            s22 = AuxConv(C.Convolution2D(48, 64, 5, pad=2, use_cudnn=use_cudnn))\n",
    "            s2 = Sequential(s21, s22)\n",
    "\n",
    "            # double 3x3\n",
    "            s31 = AuxConv(C.Convolution2D(input_channel, 64, 1, use_cudnn=use_cudnn))\n",
    "            s32 = AuxConv(C.Convolution2D(64, 96, 3, pad=1, use_cudnn=use_cudnn))\n",
    "            s33 = AuxConv(C.Convolution2D(96, 96, 3, pad=1, use_cudnn=use_cudnn))\n",
    "            s3 = Sequential(s31, s32, s33)\n",
    "\n",
    "            # pool\n",
    "            s4 = AuxConv(C.Convolution2D(input_channel,\n",
    "                                         pool_channel, 3, pad=1, use_cudnn=use_cudnn),\n",
    "                         pool=M.MaxPooling2D(3, 1, 1, use_cudnn=use_cudnn))\n",
    "\n",
    "            return Inception(s1, s2, s3, s4)\n",
    "\n",
    "        inception0 = Sequential(*[inception_0(input_channel, pool_channel)\n",
    "                                  for input_channel, pool_channel\n",
    "                                  in zip([192, 256, 288], [32, 64, 64])])\n",
    "\n",
    "        grid_reduction0 = Inception(\n",
    "            # strided 3x3\n",
    "            AuxConv(C.Convolution2D(288, 384, 3, 2, use_cudnn=use_cudnn)),\n",
    "            # double 3x3\n",
    "            Sequential(\n",
    "                AuxConv(C.Convolution2D(288, 64, 1, use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(64, 96, 3, pad=1, use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(96, 96, 3, 2, use_cudnn=use_cudnn))),\n",
    "            # pool\n",
    "            pool=M.MaxPooling2D(3, 2))\n",
    "\n",
    "        def inception_1(hidden_channel):\n",
    "            # 1x1\n",
    "            s1 = AuxConv(C.Convolution2D(768, 192, 1, use_cudnn=use_cudnn))\n",
    "\n",
    "            # 7x7\n",
    "            s21 = AuxConv(C.Convolution2D(768, hidden_channel, 1, use_cudnn=use_cudnn))\n",
    "            s22 = AuxConv(C.Convolution2D(hidden_channel, hidden_channel, (1, 7), pad=(0, 3), use_cudnn=use_cudnn))\n",
    "            s23 = AuxConv(C.Convolution2D(hidden_channel, 192, (7, 1), pad=(3, 0), use_cudnn=use_cudnn))\n",
    "            s2 = Sequential(s21, s22, s23)\n",
    "\n",
    "            # double 7x7\n",
    "            s31 = AuxConv(C.Convolution2D(768, hidden_channel, 1, use_cudnn=use_cudnn))\n",
    "            s32 = AuxConv(C.Convolution2D(hidden_channel, hidden_channel, (1, 7), pad=(0, 3), use_cudnn=use_cudnn))\n",
    "            s33 = AuxConv(C.Convolution2D(hidden_channel, hidden_channel, (7, 1), pad=(3, 0), use_cudnn=use_cudnn))\n",
    "            s34 = AuxConv(C.Convolution2D(hidden_channel, hidden_channel, (1, 7), pad=(0, 3), use_cudnn=use_cudnn))\n",
    "            s35 = AuxConv(C.Convolution2D(hidden_channel, 192, (7, 1), pad=(3, 0), use_cudnn=use_cudnn))\n",
    "            s3 = Sequential(s31, s32, s33, s34, s35)\n",
    "\n",
    "            # pool\n",
    "            s4 = AuxConv(C.Convolution2D(768, 192, 3, pad=1, use_cudnn=use_cudnn),\n",
    "                         pool=A.AveragePooling2D(3, 1, 1, use_cudnn=use_cudnn))\n",
    "\n",
    "            return Inception(s1, s2, s3, s4)\n",
    "\n",
    "        inception1 = Sequential(*[inception_1(c)\n",
    "                                  for c in [128, 160, 160, 192]])\n",
    "\n",
    "        grid_reduction1 = Inception(\n",
    "            # strided 3x3\n",
    "            Sequential(\n",
    "                AuxConv(C.Convolution2D(768, 192, 1, use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(192, 320, 3, 2, use_cudnn=use_cudnn))),\n",
    "            # 7x7 and 3x3\n",
    "            Sequential(\n",
    "                AuxConv(C.Convolution2D(768, 192, 1, use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(192, 192, (1, 7), pad=(0, 3), use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(192, 192, (7, 1), pad=(3, 0), use_cudnn=use_cudnn)),\n",
    "                AuxConv(C.Convolution2D(192, 192, 3, 2, use_cudnn=use_cudnn))),\n",
    "            # pool\n",
    "            pool=M.MaxPooling2D(3, 2, use_cudnn=use_cudnn))\n",
    "\n",
    "        def inception_2(input_channel):\n",
    "            # 1x1\n",
    "            s1 = AuxConv(C.Convolution2D(input_channel, 320, 1, use_cudnn=use_cudnn))\n",
    "\n",
    "            # 3x3\n",
    "            s21 = AuxConv(C.Convolution2D(input_channel, 384, 1, use_cudnn=use_cudnn))\n",
    "            s22 = Inception(AuxConv(C.Convolution2D(384, 384, (1, 3),\n",
    "                                                    pad=(0, 1), use_cudnn=use_cudnn)),\n",
    "                            AuxConv(C.Convolution2D(384, 384, (3, 1),\n",
    "                                                    pad=(1, 0), use_cudnn=use_cudnn)))\n",
    "            s2 = Sequential(s21, s22)\n",
    "\n",
    "            # double 3x3\n",
    "            s31 = AuxConv(C.Convolution2D(input_channel, 448, 1, use_cudnn=use_cudnn))\n",
    "            s32 = AuxConv(C.Convolution2D(448, 384, 3, pad=1, use_cudnn=use_cudnn))\n",
    "            s331 = AuxConv(C.Convolution2D(384, 384, (1, 3), pad=(0, 1), use_cudnn=use_cudnn))\n",
    "            s332 = AuxConv(C.Convolution2D(384, 384, (3, 1), pad=(1, 0), use_cudnn=use_cudnn))\n",
    "            s33 = Inception(s331, s332)\n",
    "            s3 = Sequential(s31, s32, s33)\n",
    "\n",
    "            # pool\n",
    "            s4 = AuxConv(C.Convolution2D(input_channel, 192, 3, pad=1, use_cudnn=use_cudnn),\n",
    "                         pool=A.AveragePooling2D(3, 1, 1, use_cudnn=use_cudnn))\n",
    "            return Inception(s1, s2, s3, s4)\n",
    "\n",
    "        inception2 = Sequential(*[inception_2(input_channel)\n",
    "                                  for input_channel in [1280, 2048]])\n",
    "\n",
    "        auxiliary_convolution = Sequential(\n",
    "            AuxConv(C.Convolution2D(768, 128, 1, use_cudnn=use_cudnn),\n",
    "                    pool=A.AveragePooling2D(5, 3, use_cudnn=use_cudnn)),\n",
    "            AuxConv(C.Convolution2D(128, 768, 5, use_cudnn=use_cudnn)))\n",
    "\n",
    "        super(InceptionV3, self).__init__(\n",
    "            convolution=convolution,\n",
    "            inception=link.ChainList(inception0, inception1, inception2),\n",
    "            grid_reduction=link.ChainList(grid_reduction0, grid_reduction1),\n",
    "            auxiliary_convolution=auxiliary_convolution,\n",
    "            auxiliary_linear=linear.Linear(768, 1000),\n",
    "            linear=linear.Linear(2048, 1000))\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        \"\"\"Computes the output of the module.\n",
    "        Args:\n",
    "            x(~chainer.Variable): Input variable.\n",
    "        \"\"\"\n",
    "\n",
    "        def convolution(x, train):\n",
    "            x = self.convolution[0](x)\n",
    "            x = self.convolution[1](x)\n",
    "            x = self.convolution[2](x)\n",
    "            x = M.max_pooling_2d(x, 3, 2)\n",
    "            x = self.convolution[3](x)\n",
    "            x = self.convolution[4](x)\n",
    "            x = M.max_pooling_2d(x, 3, 2)\n",
    "            return x\n",
    "\n",
    "        # Original paper and TensorFlow implementation has different\n",
    "        # auxiliary classifier. We implement latter one.\n",
    "        def auxiliary_classifier(x, train):\n",
    "            x = self.auxiliary_convolution(x, train)\n",
    "            return self.auxiliary_linear(x)\n",
    "\n",
    "        def classifier(x, train):\n",
    "            x = A.average_pooling_2d(x, 8)\n",
    "            x = dropout.dropout(x, train=train)\n",
    "            x = self.linear(x)\n",
    "            return x\n",
    "\n",
    "        x = convolution(x, train)\n",
    "        assert x.data.shape[1:] == (192, 35, 35),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        x = self.inception[0](x, train)\n",
    "        assert x.data.shape[1:] == (288, 35, 35),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        x = self.grid_reduction[0](x, train)\n",
    "        assert x.data.shape[1:] == (768, 17, 17),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        x = self.inception[1](x, train)\n",
    "        assert x.data.shape[1:] == (768, 17, 17),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        y_aux = auxiliary_classifier(x, train)\n",
    "        x = self.grid_reduction[1](x, train)\n",
    "        assert x.data.shape[1:] == (1280, 8, 8),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        x = self.inception[2](x, train)\n",
    "        assert x.data.shape[1:] == (2048, 8, 8),\\\n",
    "            'actual={}'.format(x.data.shape[1:])\n",
    "        y = classifier(x, train)\n",
    "        self.y = y\n",
    "        self.y_aux = y_aux\n",
    "        return y, y_aux\n",
    "\n",
    "\n",
    "class InceptionV3Classifier(link.Chain):\n",
    "\n",
    "    def __init__(self, predictor):\n",
    "        super(InceptionV3Classifier, self).__init__(predictor=predictor)\n",
    "\n",
    "    def __call__(self, *args):\n",
    "\n",
    "        assert len(args) >= 2\n",
    "        x = args[:-1]\n",
    "        t = args[-1]\n",
    "        self.y, self.y_aux = self.predictor(*x)\n",
    "        self.loss = S.softmax_cross_entropy(self.y, t)\n",
    "        self.loss += S.softmax_cross_entropy(self.y_aux, t)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- SqueezeNet---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open source: https://github.com/akashsunilgaikwad/Pytorch-Squeeznet\n",
    "# Title: Implementation of Squeezenet in pytorch using CIFAR-10 Dataset , Accuracy of model=66%\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Mode SqueeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Fire Module Layer\n",
    "'''\n",
    "class fire(nn.Module):\n",
    "    def __init__(self, inplanes, squeeze_planes, expand_planes):\n",
    "        super(fire, self).__init__()\n",
    "        # Kernel 1x1\n",
    "        self.conv1 = nn.Conv2d(inplanes, \n",
    "                               squeeze_planes, \n",
    "                               kernel_size=1, \n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(squeeze_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #Kernel 1x1 and 3x3\n",
    "        self.conv2 = nn.Conv2d(squeeze_planes, \n",
    "                               expand_planes, \n",
    "                               kernel_size=1,\n",
    "                               stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(expand_planes)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(squeeze_planes,\n",
    "                              expand_planes,\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(expand_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Using MSR Init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        out1 = self.conv2(x)\n",
    "        out1 = self.bn2(out1)\n",
    "        out2 = self.conv3(x)\n",
    "        out2 = self.bn3(out2)\n",
    "        out = torch.cat([out1, out2] , 1)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "'''\n",
    "SqueezeNet Layer\n",
    "'''\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        '''\n",
    "            3 is number of Input channels\n",
    "            96 is number of Output channels\n",
    "        '''\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1) #32\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) #16\n",
    "        '''\n",
    "        input planes: 96\n",
    "        squeeze_planes input: 16\n",
    "        expand_planes input: 64\n",
    "        '''\n",
    "        self.fire2 = fire(96,16,64)\n",
    "        self.fire3 = fire(128, 16, 64)\n",
    "        self.fire4 = fire(128, 32, 128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 8\n",
    "        self.fire5 = fire(256, 32, 128)\n",
    "        self.fire6 = fire(256, 48, 192)\n",
    "        self.fire7 = fire(384, 48, 192)\n",
    "        self.fire8 = fire(384, 64, 256)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 4\n",
    "        self.fire9 = fire(512, 64, 256)\n",
    "        self.conv2 = nn.Conv2d(512, 10, kernel_size=1, stride=1)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "        # Using MSR initilization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def fire_layer(inp, s, e):\n",
    "        f = fire(inp, s, e)\n",
    "        return f\n",
    "        \n",
    "    def squeezenet(pretrained= False):\n",
    "        net = SqueezeNet()\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.491399689874, 0.482158419622, 0.446530924224), (0.247032237587, 0.243485133253, 0.261587846975))])\n",
    "\n",
    "train_set = datasets.CIFAR10(root='data/cifar-10-batches-py', train=True, download=True, transform=transform)\n",
    "\n",
    "test_set = datasets.CIFAR10(root='data/cifar-10-batches-py', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Training\n",
    "n_training_samples = 50000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "print(train_sampler)\n",
    "\n",
    "\n",
    "# Test\n",
    "n_test_samples = 10000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "\n",
    "# Init Figure\n",
    "avg_loss = list()\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "#---------------------------TRAINING FUNCTION----------------------\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    global avg_loss\n",
    "    model.train()   \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.view(len(target),-1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Conpute Average Loss\n",
    "        avg_loss.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            ax1.plot(avg_loss)\n",
    "            fig1.savefig(\"Net_loss.jpg\")\n",
    "            \n",
    "            \n",
    "#-----------------------------TEST FUNCTION-------------------------\n",
    "def test(model, device, test_loader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = output.view(len(target),-1)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "\n",
    "#----------------------------MAIN----------------------------------\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # -----------------  Training settings  ---------------------\n",
    "    # Use CUDA for traning or not\n",
    "    no_cuda = False\n",
    "\n",
    "    # Batch size of training\n",
    "    batch_size = 64 \n",
    "\n",
    "    # Number of epochs to train for\n",
    "    epochs = 55\n",
    "\n",
    "    # Learning Rate\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # With 10 epoch should be store state of the result once\n",
    "    log_interval = 10\n",
    "\n",
    "    # Percentage of past params to store \n",
    "    momentum = 0.9\n",
    "\n",
    "    # Set seed to some constant value to reproduce experiments\n",
    "    seed = 42\n",
    "\n",
    "    # Data contain Classes as\n",
    "    classes = ('plane', 'car', 'bird', 'cat','deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    # Check having GPU or not\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    # Split Train and test data\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
    "    print(len(train_loader.dataset))\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "    # Init Model\n",
    "    model = SqueezeNet().to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Init Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    \n",
    "    # *************** Start Training model over each Epoch************\n",
    "    since = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "        torch.save(model, \"akash_squeezenet\")\n",
    "    \n",
    "    # Total Time for Training using GPU    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # ***************Start Test model From test Dataset***************\n",
    "    test(model, device, test_loader)\n",
    "    \n",
    "\n",
    "    # Get some image from test dataset\n",
    "    dataiter = iter(test_loader)\n",
    "    print(test_loader)\n",
    "    images, labels = dataiter.next()\n",
    "    images, labels = Variable(images), Variable(labels)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    print(labels)\n",
    "    \n",
    "    print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(12)))\n",
    "    \n",
    "    # Feed images to Model to predict\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # Just get 12 result, which mean is maximun result\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(12)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# -------------------- Pruning VGG 16 -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import dataset\n",
    "#from prune import *\n",
    "import argparse\n",
    "from operator import itemgetter\n",
    "from heapq import nsmallest\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prunningしたい学習済みモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModifiedVGG16Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        self.features = model.features\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = ModifiedVGG16Model().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの準備\n",
    "本実装では、　データ・セットは:\n",
    "\n",
    "     * 犬猫の2クラス分類\n",
    "         * トレーニングデータ1000枚ずつ\n",
    "         * テストデータ400枚ずつ\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n"
     ]
    }
   ],
   "source": [
    "train_path = \"data/dac_train\"\n",
    "test_path = \"data/dac_test\"\n",
    "\n",
    "train_data_loader = dataset.loader(train_path)\n",
    "test_data_loader = dataset.loader(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各フィルタの重要度を計算\n",
    "\n",
    "1・**各フィルタの重要度GRAD x Activation**　を計算し、辞書Filter_Ranksに保存する。\n",
    "\n",
    "２．Register_Hookを用いて、Backwardの際に重要度を計算するようにする。\n",
    "\n",
    "３．勾配と特徴マップからフィルタの重要度を返す関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Function for Register_hook\n",
    "    Compute Grad * Activation\n",
    "''' \n",
    "def compute_rank(grad):\n",
    "    global grad_index \n",
    "    global activations\n",
    "    global filter_rank\n",
    "    \n",
    "    activation_index = len(activations) - grad_index - 1\n",
    "    activation = activations[activation_index]\n",
    "    \n",
    "    values = torch.sum((activation * grad), dim = 0, keepdim = True).sum(dim=2, keepdim=True).sum(dim=3, keepdim=True)[0, :, 0, 0].data\n",
    "\n",
    "    # Normalize the rank by the filter dimensions\n",
    "    values = values / (activation.size(0) * activation.size(2) * activation.size(3))\n",
    "    \n",
    "    if activation_index not in filter_ranks:\n",
    "        filter_ranks[activation_index] = torch.FloatTensor(activation.size(1)).zero_().cuda()\n",
    "\n",
    "    filter_ranks[activation_index] += values\n",
    "    grad_index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フィルタの重要度を計算するループ\n",
    "* 順伝播の際にactivationを記録、各層の出力xにcompute_rankのフックを登録\n",
    "* 逆伝播の際にcompute_rankが呼び出されgrad*activationを計算、filter_ranksに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 80000 bytes but only got 0. Skipping tag 64640\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 6553600 bytes but only got 0. Skipping tag 49\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 1050744 bytes but only got 4951. Skipping tag 51\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 293339136 bytes but only got 0. Skipping tag 5\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 293863424 bytes but only got 0. Skipping tag 5\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 3368026112 bytes but only got 0. Skipping tag 7\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 134479872 bytes but only got 0. Skipping tag 7\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 295698432 bytes but only got 0. Skipping tag 10\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 296222720 bytes but only got 0. Skipping tag 5\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 3300917248 bytes but only got 0. Skipping tag 7\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 65536 bytes but only got 0. Skipping tag 3\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 14745600 bytes but only got 0. Skipping tag 4\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 25624576 bytes but only got 0. Skipping tag 4\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 317718528 bytes but only got 4956. Skipping tag 4\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 131073 bytes but only got 4952. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 393216 bytes but only got 0. Skipping tag 3\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 287178752 bytes but only got 0. Skipping tag 5\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 287703040 bytes but only got 0. Skipping tag 5\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 131072 bytes but only got 0. Skipping tag 3\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 524288 bytes but only got 0. Skipping tag 4\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:747: UserWarning: Possibly corrupt EXIF data.  Expecting to read 286654464 bytes but only got 4956. Skipping tag 4\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:764: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 2. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2622, in open\n    % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='data/dac_train/Cat/666.jpg'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-21d1f56f1337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# back propagation loop computing importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Traceback (most recent call last):\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/home/hoanghuy/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2622, in open\n    % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='data/dac_train/Cat/666.jpg'>\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# dictonary storing importances of each filter\n",
    "filter_ranks = {}\n",
    "\n",
    "# back propagation loop computing importances\n",
    "for batch, label in train_data_loader:\n",
    "    model.zero_grad()\n",
    "    x = Variable(batch.cuda(), requires_grad = True)\n",
    "\n",
    "    activations = []\n",
    "    grad_index = 0\n",
    "    activation_to_layer = {}\n",
    "    activation_index = 0\n",
    "\n",
    "    # forward\n",
    "    for layer, (name, module) in enumerate(model.features._modules.items()):\n",
    "        # print(\"layer, name, module: \", layer, name, module)\n",
    "\n",
    "        x = module(x)\n",
    "        \n",
    "        # store activation\n",
    "        if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "            x.register_hook(compute_rank)\n",
    "            activations.append(x)\n",
    "            activation_to_layer[activation_index] = layer\n",
    "            activation_index += 1\n",
    "\n",
    "    # print(x.data.cpu().numpy().shape)\n",
    "    out = model.classifier(x.view(x.size(0), -1))\n",
    "\n",
    "    # backward, compute compute_rank()\n",
    "    # no update of weights\n",
    "    criterion(out, Variable(label.cuda())).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer, name, module:  0 0 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  1 1 ReLU(inplace)\n",
      "layer, name, module:  2 2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  3 3 ReLU(inplace)\n",
      "layer, name, module:  4 4 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "layer, name, module:  5 5 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  6 6 ReLU(inplace)\n",
      "layer, name, module:  7 7 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  8 8 ReLU(inplace)\n",
      "layer, name, module:  9 9 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "layer, name, module:  10 10 Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  11 11 ReLU(inplace)\n",
      "layer, name, module:  12 12 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  13 13 ReLU(inplace)\n",
      "layer, name, module:  14 14 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  15 15 ReLU(inplace)\n",
      "layer, name, module:  16 16 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "layer, name, module:  17 17 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  18 18 ReLU(inplace)\n",
      "layer, name, module:  19 19 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  20 20 ReLU(inplace)\n",
      "layer, name, module:  21 21 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  22 22 ReLU(inplace)\n",
      "layer, name, module:  23 23 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "layer, name, module:  24 24 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  25 25 ReLU(inplace)\n",
      "layer, name, module:  26 26 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  27 27 ReLU(inplace)\n",
      "layer, name, module:  28 28 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "layer, name, module:  29 29 ReLU(inplace)\n",
      "layer, name, module:  30 30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for layer, (name, module) in enumerate(model.features._modules.items()):\n",
    "    print(\"layer, name, module: \", layer, name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6822e-08, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(filter_ranks[2][10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 (512,) [-6.0348907e-07 -2.0678119e-06  5.6524567e-07  5.6731360e-06\n",
      "  1.2251176e-06 -1.2918512e-06 -5.4271641e-07 -1.7050613e-06\n",
      "  2.2381900e-07 -4.0480626e-07]\n",
      "11 (512,) [-4.0273159e-07  6.5658298e-07 -2.9559519e-07  7.3209719e-07\n",
      " -2.5223716e-08  1.4423428e-06 -1.0417439e-06  9.9836654e-07\n",
      " -4.6311379e-07  3.9538892e-07]\n",
      "10 (512,) [ 1.0242868e-06 -2.6958023e-07 -3.1653653e-07  6.8438517e-08\n",
      " -3.4067494e-07  7.6176616e-07  1.6274830e-06 -1.7018294e-06\n",
      " -9.5201739e-07 -1.4180220e-06]\n",
      "9 (512,) [ 4.2558969e-07 -4.4229278e-08 -5.0588017e-07 -3.0120439e-08\n",
      " -5.5878371e-07 -1.1650202e-07  5.7924424e-08  4.9158865e-07\n",
      "  4.3449066e-07  3.3939210e-07]\n",
      "8 (512,) [ 3.5146655e-08  1.4222414e-07  7.6658388e-08  2.9983207e-07\n",
      " -8.5055035e-08  4.0483499e-07 -2.0752111e-10  1.0864391e-07\n",
      "  1.2579569e-07 -2.7304182e-07]\n",
      "7 (512,) [-2.7931810e-07 -1.0016723e-06  1.4125433e-07 -3.2699293e-07\n",
      " -3.1640599e-07 -2.3698078e-08  4.1145614e-07 -7.7593498e-08\n",
      "  7.7515637e-08 -1.7111345e-08]\n",
      "6 (256,) [ 1.22098385e-08 -6.80864076e-08 -1.17956134e-08 -2.51072354e-08\n",
      " -1.62849631e-07 -9.54407113e-08  1.86608176e-07  8.07320077e-08\n",
      " -1.28035481e-07  3.76348126e-08]\n",
      "5 (256,) [-7.9911487e-08 -2.1798868e-08  6.1982334e-08 -5.2174716e-08\n",
      "  2.7511919e-07  1.4058051e-07  1.1070933e-09 -2.2971109e-08\n",
      " -3.0665021e-08  5.3277990e-08]\n",
      "4 (256,) [-3.76942388e-08 -7.94462949e-08  3.84273413e-08 -1.75040469e-07\n",
      " -9.20595085e-08 -1.27470310e-08 -5.67630210e-08 -1.88922343e-07\n",
      "  1.03907006e-07  1.32476515e-07]\n",
      "3 (128,) [-1.9884380e-08  8.5277286e-08  4.7026052e-08 -1.1866533e-08\n",
      " -4.0076991e-08 -1.7903915e-08  2.3333770e-08  9.1456279e-08\n",
      " -1.7330215e-08  3.0595224e-09]\n",
      "2 (128,) [ 5.8080349e-09 -1.8059344e-08  1.9364457e-08  1.8452464e-08\n",
      " -2.6533835e-09  2.0385421e-08  1.6216569e-08  3.6456993e-09\n",
      " -1.5372770e-09  8.0162330e-09]\n",
      "1 (64,) [ 8.4624849e-09 -3.4291943e-09 -1.8690512e-08  1.6451768e-09\n",
      "  3.4459784e-08 -6.5186541e-09 -6.9417090e-09  4.3685637e-09\n",
      "  1.3539255e-08  7.8881490e-10]\n",
      "0 (64,) [ 3.1811115e-10 -4.1034061e-09  7.1892208e-09  8.7246219e-09\n",
      " -4.5779313e-08 -2.6744171e-09  1.8098767e-09 -3.8048391e-09\n",
      " -1.0683396e-08 -2.5086619e-08]\n"
     ]
    }
   ],
   "source": [
    "for i in filter_ranks:\n",
    "    f = filter_ranks[i].cpu().numpy()\n",
    "    print(i, f.shape, f[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
